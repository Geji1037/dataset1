{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 1.9051178693771362,
      "learning_rate": 0.000298,
      "loss": 1.5559,
      "step": 1
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.6617887020111084,
      "learning_rate": 0.000296,
      "loss": 1.1098,
      "step": 2
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.541440725326538,
      "learning_rate": 0.000294,
      "loss": 1.3375,
      "step": 3
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.7519360780715942,
      "learning_rate": 0.000292,
      "loss": 1.4431,
      "step": 4
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.5055705308914185,
      "learning_rate": 0.00029,
      "loss": 1.4356,
      "step": 5
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.823143482208252,
      "learning_rate": 0.00028799999999999995,
      "loss": 1.3933,
      "step": 6
    },
    {
      "epoch": 0.14,
      "grad_norm": 2.1628246307373047,
      "learning_rate": 0.00028599999999999996,
      "loss": 1.3166,
      "step": 7
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.7402557134628296,
      "learning_rate": 0.00028399999999999996,
      "loss": 1.4276,
      "step": 8
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.8496779203414917,
      "learning_rate": 0.00028199999999999997,
      "loss": 1.3758,
      "step": 9
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.8289462327957153,
      "learning_rate": 0.00028,
      "loss": 1.1486,
      "step": 10
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.6950712203979492,
      "learning_rate": 0.000278,
      "loss": 1.5262,
      "step": 11
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.4243911504745483,
      "learning_rate": 0.000276,
      "loss": 1.1524,
      "step": 12
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.7806981801986694,
      "learning_rate": 0.000274,
      "loss": 1.4531,
      "step": 13
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.5072660446166992,
      "learning_rate": 0.00027199999999999994,
      "loss": 1.425,
      "step": 14
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.6927978992462158,
      "learning_rate": 0.00027,
      "loss": 1.4498,
      "step": 15
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.4195480346679688,
      "learning_rate": 0.00026799999999999995,
      "loss": 1.3391,
      "step": 16
    },
    {
      "epoch": 0.34,
      "grad_norm": 2.0799920558929443,
      "learning_rate": 0.000266,
      "loss": 1.43,
      "step": 17
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.4557523727416992,
      "learning_rate": 0.00026399999999999997,
      "loss": 1.3189,
      "step": 18
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.3558555841445923,
      "learning_rate": 0.00026199999999999997,
      "loss": 1.2964,
      "step": 19
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.5540026426315308,
      "learning_rate": 0.00026,
      "loss": 1.2053,
      "step": 20
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.414190649986267,
      "learning_rate": 0.000258,
      "loss": 1.4083,
      "step": 21
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.5916396379470825,
      "learning_rate": 0.000256,
      "loss": 1.4275,
      "step": 22
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.5995434522628784,
      "learning_rate": 0.000254,
      "loss": 1.3655,
      "step": 23
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.5695836544036865,
      "learning_rate": 0.00025199999999999995,
      "loss": 1.2404,
      "step": 24
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.2765498161315918,
      "learning_rate": 0.00025,
      "loss": 1.5074,
      "step": 25
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.8239116668701172,
      "learning_rate": 0.00024799999999999996,
      "loss": 1.4959,
      "step": 26
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.4466572999954224,
      "learning_rate": 0.00024599999999999996,
      "loss": 1.2978,
      "step": 27
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.8447818756103516,
      "learning_rate": 0.000244,
      "loss": 1.2308,
      "step": 28
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.583556056022644,
      "learning_rate": 0.00024199999999999997,
      "loss": 1.6589,
      "step": 29
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.1230621337890625,
      "learning_rate": 0.00023999999999999998,
      "loss": 1.2404,
      "step": 30
    },
    {
      "epoch": 0.62,
      "grad_norm": 1.8868498802185059,
      "learning_rate": 0.00023799999999999998,
      "loss": 1.27,
      "step": 31
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.4672411680221558,
      "learning_rate": 0.00023599999999999996,
      "loss": 1.1779,
      "step": 32
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.3190598487854004,
      "learning_rate": 0.000234,
      "loss": 1.3442,
      "step": 33
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.5337116718292236,
      "learning_rate": 0.00023199999999999997,
      "loss": 1.3785,
      "step": 34
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.6872873306274414,
      "learning_rate": 0.00023,
      "loss": 1.4178,
      "step": 35
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.5102949142456055,
      "learning_rate": 0.00022799999999999999,
      "loss": 1.3335,
      "step": 36
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.4318873882293701,
      "learning_rate": 0.00022599999999999996,
      "loss": 1.2494,
      "step": 37
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.5817112922668457,
      "learning_rate": 0.000224,
      "loss": 1.3348,
      "step": 38
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.8883768320083618,
      "learning_rate": 0.00022199999999999998,
      "loss": 1.1503,
      "step": 39
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.5420066118240356,
      "learning_rate": 0.00021999999999999995,
      "loss": 1.4636,
      "step": 40
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.2983721494674683,
      "learning_rate": 0.00021799999999999999,
      "loss": 1.3667,
      "step": 41
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.5493110418319702,
      "learning_rate": 0.00021599999999999996,
      "loss": 1.2711,
      "step": 42
    },
    {
      "epoch": 0.86,
      "grad_norm": 2.1399800777435303,
      "learning_rate": 0.000214,
      "loss": 1.2935,
      "step": 43
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.1980705261230469,
      "learning_rate": 0.00021199999999999998,
      "loss": 1.2583,
      "step": 44
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.708884358406067,
      "learning_rate": 0.00020999999999999998,
      "loss": 1.3036,
      "step": 45
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.3335527181625366,
      "learning_rate": 0.000208,
      "loss": 1.2554,
      "step": 46
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.2727434635162354,
      "learning_rate": 0.00020599999999999997,
      "loss": 1.3707,
      "step": 47
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.4601308107376099,
      "learning_rate": 0.000204,
      "loss": 1.3971,
      "step": 48
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.5394326448440552,
      "learning_rate": 0.00020199999999999998,
      "loss": 1.3687,
      "step": 49
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3743230104446411,
      "learning_rate": 0.00019999999999999998,
      "loss": 1.388,
      "step": 50
    },
    {
      "epoch": 1.02,
      "grad_norm": 1.749294400215149,
      "learning_rate": 0.000198,
      "loss": 1.0768,
      "step": 51
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.5547139644622803,
      "learning_rate": 0.00019599999999999997,
      "loss": 1.0814,
      "step": 52
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.4250444173812866,
      "learning_rate": 0.00019399999999999997,
      "loss": 1.3453,
      "step": 53
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.2951833009719849,
      "learning_rate": 0.00019199999999999998,
      "loss": 1.3845,
      "step": 54
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.3945276737213135,
      "learning_rate": 0.00018999999999999998,
      "loss": 0.9712,
      "step": 55
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.3420387506484985,
      "learning_rate": 0.000188,
      "loss": 1.2102,
      "step": 56
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.9097052812576294,
      "learning_rate": 0.000186,
      "loss": 1.193,
      "step": 57
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.3280210494995117,
      "learning_rate": 0.00018399999999999997,
      "loss": 1.1105,
      "step": 58
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.8287712335586548,
      "learning_rate": 0.00018199999999999998,
      "loss": 1.2844,
      "step": 59
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.3606159687042236,
      "learning_rate": 0.00017999999999999998,
      "loss": 1.1498,
      "step": 60
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.6579457521438599,
      "learning_rate": 0.000178,
      "loss": 1.1644,
      "step": 61
    },
    {
      "epoch": 1.24,
      "grad_norm": 2.040330410003662,
      "learning_rate": 0.000176,
      "loss": 1.0829,
      "step": 62
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.7906243801116943,
      "learning_rate": 0.00017399999999999997,
      "loss": 1.0179,
      "step": 63
    },
    {
      "epoch": 1.28,
      "grad_norm": 2.0893330574035645,
      "learning_rate": 0.000172,
      "loss": 1.247,
      "step": 64
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.564888596534729,
      "learning_rate": 0.00016999999999999999,
      "loss": 1.289,
      "step": 65
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.8849372863769531,
      "learning_rate": 0.000168,
      "loss": 1.3185,
      "step": 66
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.6972578763961792,
      "learning_rate": 0.000166,
      "loss": 1.248,
      "step": 67
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 2.0546939373016357,
      "learning_rate": 0.00016399999999999997,
      "loss": 1.2635,
      "step": 68
    },
    {
      "epoch": 1.38,
      "grad_norm": 2.17427921295166,
      "learning_rate": 0.000162,
      "loss": 0.9451,
      "step": 69
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.090630054473877,
      "learning_rate": 0.00015999999999999999,
      "loss": 1.1259,
      "step": 70
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.6548302173614502,
      "learning_rate": 0.00015799999999999996,
      "loss": 1.1324,
      "step": 71
    },
    {
      "epoch": 1.44,
      "grad_norm": 2.152315855026245,
      "learning_rate": 0.000156,
      "loss": 1.1782,
      "step": 72
    },
    {
      "epoch": 1.46,
      "grad_norm": 2.0759165287017822,
      "learning_rate": 0.00015399999999999998,
      "loss": 1.1273,
      "step": 73
    },
    {
      "epoch": 1.48,
      "grad_norm": 2.0891706943511963,
      "learning_rate": 0.000152,
      "loss": 1.2654,
      "step": 74
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.8846068382263184,
      "learning_rate": 0.00015,
      "loss": 1.2811,
      "step": 75
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.7618225812911987,
      "learning_rate": 0.000148,
      "loss": 1.0053,
      "step": 76
    },
    {
      "epoch": 1.54,
      "grad_norm": 2.5635225772857666,
      "learning_rate": 0.000146,
      "loss": 1.0756,
      "step": 77
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.6423959732055664,
      "learning_rate": 0.00014399999999999998,
      "loss": 1.2658,
      "step": 78
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.7668789625167847,
      "learning_rate": 0.00014199999999999998,
      "loss": 1.1075,
      "step": 79
    },
    {
      "epoch": 1.6,
      "grad_norm": 2.135719060897827,
      "learning_rate": 0.00014,
      "loss": 1.1251,
      "step": 80
    },
    {
      "epoch": 1.62,
      "grad_norm": 2.16819429397583,
      "learning_rate": 0.000138,
      "loss": 1.0715,
      "step": 81
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.7189213037490845,
      "learning_rate": 0.00013599999999999997,
      "loss": 1.2826,
      "step": 82
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.72952139377594,
      "learning_rate": 0.00013399999999999998,
      "loss": 1.1938,
      "step": 83
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 2.069608449935913,
      "learning_rate": 0.00013199999999999998,
      "loss": 1.1949,
      "step": 84
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.7231589555740356,
      "learning_rate": 0.00013,
      "loss": 1.1392,
      "step": 85
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.9648187160491943,
      "learning_rate": 0.000128,
      "loss": 1.1247,
      "step": 86
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.695061206817627,
      "learning_rate": 0.00012599999999999997,
      "loss": 1.1997,
      "step": 87
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.973254680633545,
      "learning_rate": 0.00012399999999999998,
      "loss": 1.2447,
      "step": 88
    },
    {
      "epoch": 1.78,
      "grad_norm": 1.578597903251648,
      "learning_rate": 0.000122,
      "loss": 1.0418,
      "step": 89
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.048917055130005,
      "learning_rate": 0.00011999999999999999,
      "loss": 1.2258,
      "step": 90
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.6019535064697266,
      "learning_rate": 0.00011799999999999998,
      "loss": 1.2741,
      "step": 91
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.6001657247543335,
      "learning_rate": 0.00011599999999999999,
      "loss": 0.9916,
      "step": 92
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 2.2593233585357666,
      "learning_rate": 0.00011399999999999999,
      "loss": 1.3838,
      "step": 93
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.787736415863037,
      "learning_rate": 0.000112,
      "loss": 1.0491,
      "step": 94
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.6352828741073608,
      "learning_rate": 0.00010999999999999998,
      "loss": 1.1089,
      "step": 95
    },
    {
      "epoch": 1.92,
      "grad_norm": 2.021512031555176,
      "learning_rate": 0.00010799999999999998,
      "loss": 1.1973,
      "step": 96
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.7173006534576416,
      "learning_rate": 0.00010599999999999999,
      "loss": 1.0598,
      "step": 97
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.972298502922058,
      "learning_rate": 0.000104,
      "loss": 1.228,
      "step": 98
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.9380680322647095,
      "learning_rate": 0.000102,
      "loss": 1.3243,
      "step": 99
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.7808576822280884,
      "learning_rate": 9.999999999999999e-05,
      "loss": 0.9554,
      "step": 100
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.5670839548110962,
      "learning_rate": 9.799999999999998e-05,
      "loss": 1.1024,
      "step": 101
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.8697624206542969,
      "learning_rate": 9.599999999999999e-05,
      "loss": 0.9744,
      "step": 102
    },
    {
      "epoch": 2.06,
      "grad_norm": 2.1461246013641357,
      "learning_rate": 9.4e-05,
      "loss": 0.9525,
      "step": 103
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.8452016115188599,
      "learning_rate": 9.199999999999999e-05,
      "loss": 0.7739,
      "step": 104
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.9021481275558472,
      "learning_rate": 8.999999999999999e-05,
      "loss": 1.1736,
      "step": 105
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.59174382686615,
      "learning_rate": 8.8e-05,
      "loss": 0.9298,
      "step": 106
    },
    {
      "epoch": 2.14,
      "grad_norm": 1.8466166257858276,
      "learning_rate": 8.6e-05,
      "loss": 0.9552,
      "step": 107
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.8523004055023193,
      "learning_rate": 8.4e-05,
      "loss": 1.2947,
      "step": 108
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.9509215354919434,
      "learning_rate": 8.199999999999999e-05,
      "loss": 1.1155,
      "step": 109
    },
    {
      "epoch": 2.2,
      "grad_norm": 2.0482821464538574,
      "learning_rate": 7.999999999999999e-05,
      "loss": 1.0793,
      "step": 110
    },
    {
      "epoch": 2.22,
      "grad_norm": 1.9706207513809204,
      "learning_rate": 7.8e-05,
      "loss": 1.0273,
      "step": 111
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.9589896202087402,
      "learning_rate": 7.6e-05,
      "loss": 1.2722,
      "step": 112
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.951536774635315,
      "learning_rate": 7.4e-05,
      "loss": 1.0497,
      "step": 113
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 2.0976572036743164,
      "learning_rate": 7.199999999999999e-05,
      "loss": 1.0156,
      "step": 114
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.8728076219558716,
      "learning_rate": 7e-05,
      "loss": 0.9132,
      "step": 115
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.9008736610412598,
      "learning_rate": 6.799999999999999e-05,
      "loss": 1.016,
      "step": 116
    },
    {
      "epoch": 2.34,
      "grad_norm": 2.4134490489959717,
      "learning_rate": 6.599999999999999e-05,
      "loss": 0.9241,
      "step": 117
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.9329718351364136,
      "learning_rate": 6.4e-05,
      "loss": 1.0376,
      "step": 118
    },
    {
      "epoch": 2.38,
      "grad_norm": 2.1040546894073486,
      "learning_rate": 6.199999999999999e-05,
      "loss": 1.1127,
      "step": 119
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.347031593322754,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 1.2872,
      "step": 120
    },
    {
      "epoch": 2.42,
      "grad_norm": 2.613231658935547,
      "learning_rate": 5.7999999999999994e-05,
      "loss": 0.8046,
      "step": 121
    },
    {
      "epoch": 2.44,
      "grad_norm": 2.3607730865478516,
      "learning_rate": 5.6e-05,
      "loss": 1.172,
      "step": 122
    },
    {
      "epoch": 2.46,
      "grad_norm": 2.276017189025879,
      "learning_rate": 5.399999999999999e-05,
      "loss": 1.0408,
      "step": 123
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.063601016998291,
      "learning_rate": 5.2e-05,
      "loss": 0.868,
      "step": 124
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.931628704071045,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 1.0925,
      "step": 125
    },
    {
      "epoch": 2.52,
      "grad_norm": 2.0681862831115723,
      "learning_rate": 4.7999999999999994e-05,
      "loss": 1.1263,
      "step": 126
    },
    {
      "epoch": 2.54,
      "grad_norm": 2.038336753845215,
      "learning_rate": 4.599999999999999e-05,
      "loss": 0.9659,
      "step": 127
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.9161033630371094,
      "learning_rate": 4.4e-05,
      "loss": 1.2043,
      "step": 128
    },
    {
      "epoch": 2.58,
      "grad_norm": 1.8118181228637695,
      "learning_rate": 4.2e-05,
      "loss": 1.1086,
      "step": 129
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.7665293216705322,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 0.8888,
      "step": 130
    },
    {
      "epoch": 2.62,
      "grad_norm": 2.244525909423828,
      "learning_rate": 3.8e-05,
      "loss": 0.8887,
      "step": 131
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.912872552871704,
      "learning_rate": 3.5999999999999994e-05,
      "loss": 1.0065,
      "step": 132
    },
    {
      "epoch": 2.66,
      "grad_norm": 2.133063554763794,
      "learning_rate": 3.399999999999999e-05,
      "loss": 1.0729,
      "step": 133
    },
    {
      "epoch": 2.68,
      "grad_norm": 2.297030210494995,
      "learning_rate": 3.2e-05,
      "loss": 1.0518,
      "step": 134
    },
    {
      "epoch": 2.7,
      "grad_norm": 2.032099723815918,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 1.0479,
      "step": 135
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.9484535455703735,
      "learning_rate": 2.8e-05,
      "loss": 0.7524,
      "step": 136
    },
    {
      "epoch": 2.74,
      "grad_norm": 2.447563409805298,
      "learning_rate": 2.6e-05,
      "loss": 1.1853,
      "step": 137
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.875861644744873,
      "learning_rate": 2.3999999999999997e-05,
      "loss": 0.9583,
      "step": 138
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 2.5449633598327637,
      "learning_rate": 2.2e-05,
      "loss": 0.9949,
      "step": 139
    },
    {
      "epoch": 2.8,
      "grad_norm": 2.938136339187622,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 1.1636,
      "step": 140
    },
    {
      "epoch": 2.82,
      "grad_norm": 2.003418445587158,
      "learning_rate": 1.7999999999999997e-05,
      "loss": 1.3073,
      "step": 141
    },
    {
      "epoch": 2.84,
      "grad_norm": 2.1179051399230957,
      "learning_rate": 1.6e-05,
      "loss": 1.1151,
      "step": 142
    },
    {
      "epoch": 2.86,
      "grad_norm": 2.0485153198242188,
      "learning_rate": 1.4e-05,
      "loss": 1.2079,
      "step": 143
    },
    {
      "epoch": 2.88,
      "grad_norm": 2.063627004623413,
      "learning_rate": 1.1999999999999999e-05,
      "loss": 1.0827,
      "step": 144
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.9885669946670532,
      "learning_rate": 9.999999999999999e-06,
      "loss": 1.0285,
      "step": 145
    },
    {
      "epoch": 2.92,
      "grad_norm": 2.3871512413024902,
      "learning_rate": 8e-06,
      "loss": 1.0049,
      "step": 146
    },
    {
      "epoch": 2.94,
      "grad_norm": 1.929835319519043,
      "learning_rate": 5.999999999999999e-06,
      "loss": 0.961,
      "step": 147
    },
    {
      "epoch": 2.96,
      "grad_norm": 2.2320680618286133,
      "learning_rate": 4e-06,
      "loss": 1.1044,
      "step": 148
    },
    {
      "epoch": 2.98,
      "grad_norm": 2.093832015991211,
      "learning_rate": 2e-06,
      "loss": 1.1629,
      "step": 149
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.5132646560668945,
      "learning_rate": 0.0,
      "loss": 0.9492,
      "step": 150
    }
  ],
  "logging_steps": 1,
  "max_steps": 150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1205555621986304e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}

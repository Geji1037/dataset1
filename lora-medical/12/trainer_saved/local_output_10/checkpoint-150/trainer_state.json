{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 0.4822564423084259,
      "learning_rate": 0.000298,
      "loss": 1.5384,
      "step": 1
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.6631584167480469,
      "learning_rate": 0.000296,
      "loss": 1.9659,
      "step": 2
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.7179597616195679,
      "learning_rate": 0.000294,
      "loss": 1.6759,
      "step": 3
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.8810747861862183,
      "learning_rate": 0.000292,
      "loss": 1.764,
      "step": 4
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.8540388941764832,
      "learning_rate": 0.00029,
      "loss": 1.7488,
      "step": 5
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.1631779670715332,
      "learning_rate": 0.00028799999999999995,
      "loss": 1.7851,
      "step": 6
    },
    {
      "epoch": 0.14,
      "grad_norm": 1.1436357498168945,
      "learning_rate": 0.00028599999999999996,
      "loss": 1.7234,
      "step": 7
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.6145482063293457,
      "learning_rate": 0.00028399999999999996,
      "loss": 1.7443,
      "step": 8
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.5376832485198975,
      "learning_rate": 0.00028199999999999997,
      "loss": 1.7358,
      "step": 9
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.254965901374817,
      "learning_rate": 0.00028,
      "loss": 1.6103,
      "step": 10
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.5243607759475708,
      "learning_rate": 0.000278,
      "loss": 1.3713,
      "step": 11
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.186889410018921,
      "learning_rate": 0.000276,
      "loss": 1.4466,
      "step": 12
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.042451024055481,
      "learning_rate": 0.000274,
      "loss": 1.6338,
      "step": 13
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.1180779933929443,
      "learning_rate": 0.00027199999999999994,
      "loss": 1.6143,
      "step": 14
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.9731298685073853,
      "learning_rate": 0.00027,
      "loss": 1.4903,
      "step": 15
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.944918155670166,
      "learning_rate": 0.00026799999999999995,
      "loss": 1.1004,
      "step": 16
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.375474452972412,
      "learning_rate": 0.000266,
      "loss": 1.4318,
      "step": 17
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.11976957321167,
      "learning_rate": 0.00026399999999999997,
      "loss": 1.5666,
      "step": 18
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.4341471195220947,
      "learning_rate": 0.00026199999999999997,
      "loss": 1.6009,
      "step": 19
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.2597354650497437,
      "learning_rate": 0.00026,
      "loss": 1.5501,
      "step": 20
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.3540823459625244,
      "learning_rate": 0.000258,
      "loss": 1.6756,
      "step": 21
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.4705216884613037,
      "learning_rate": 0.000256,
      "loss": 1.422,
      "step": 22
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.0552877187728882,
      "learning_rate": 0.000254,
      "loss": 1.3425,
      "step": 23
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.0877811908721924,
      "learning_rate": 0.00025199999999999995,
      "loss": 1.6104,
      "step": 24
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.2322980165481567,
      "learning_rate": 0.00025,
      "loss": 1.5388,
      "step": 25
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.2169607877731323,
      "learning_rate": 0.00024799999999999996,
      "loss": 1.4538,
      "step": 26
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.111812710762024,
      "learning_rate": 0.00024599999999999996,
      "loss": 1.6309,
      "step": 27
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.572421669960022,
      "learning_rate": 0.000244,
      "loss": 1.4709,
      "step": 28
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.5151008367538452,
      "learning_rate": 0.00024199999999999997,
      "loss": 1.2998,
      "step": 29
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.3990809917449951,
      "learning_rate": 0.00023999999999999998,
      "loss": 1.4482,
      "step": 30
    },
    {
      "epoch": 0.62,
      "grad_norm": 1.242871880531311,
      "learning_rate": 0.00023799999999999998,
      "loss": 1.5281,
      "step": 31
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.605360746383667,
      "learning_rate": 0.00023599999999999996,
      "loss": 1.5312,
      "step": 32
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.1021366119384766,
      "learning_rate": 0.000234,
      "loss": 1.3802,
      "step": 33
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.0359019041061401,
      "learning_rate": 0.00023199999999999997,
      "loss": 1.5621,
      "step": 34
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.3444668054580688,
      "learning_rate": 0.00023,
      "loss": 1.2485,
      "step": 35
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.313332438468933,
      "learning_rate": 0.00022799999999999999,
      "loss": 1.4712,
      "step": 36
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.244240164756775,
      "learning_rate": 0.00022599999999999996,
      "loss": 1.2956,
      "step": 37
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.6280559301376343,
      "learning_rate": 0.000224,
      "loss": 1.3473,
      "step": 38
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.252100944519043,
      "learning_rate": 0.00022199999999999998,
      "loss": 1.2584,
      "step": 39
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.2907556295394897,
      "learning_rate": 0.00021999999999999995,
      "loss": 1.3921,
      "step": 40
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.2069238424301147,
      "learning_rate": 0.00021799999999999999,
      "loss": 1.554,
      "step": 41
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.1416285037994385,
      "learning_rate": 0.00021599999999999996,
      "loss": 1.3599,
      "step": 42
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.1086242198944092,
      "learning_rate": 0.000214,
      "loss": 1.4435,
      "step": 43
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.2761194705963135,
      "learning_rate": 0.00021199999999999998,
      "loss": 1.5994,
      "step": 44
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.1566168069839478,
      "learning_rate": 0.00020999999999999998,
      "loss": 1.5177,
      "step": 45
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.9918443560600281,
      "learning_rate": 0.000208,
      "loss": 1.4149,
      "step": 46
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.4061253070831299,
      "learning_rate": 0.00020599999999999997,
      "loss": 1.3367,
      "step": 47
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.2867764234542847,
      "learning_rate": 0.000204,
      "loss": 1.5431,
      "step": 48
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.0803297758102417,
      "learning_rate": 0.00020199999999999998,
      "loss": 1.5484,
      "step": 49
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0916856527328491,
      "learning_rate": 0.00019999999999999998,
      "loss": 1.4615,
      "step": 50
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.9774076342582703,
      "learning_rate": 0.000198,
      "loss": 1.3142,
      "step": 51
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.0177745819091797,
      "learning_rate": 0.00019599999999999997,
      "loss": 1.4576,
      "step": 52
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.0108755826950073,
      "learning_rate": 0.00019399999999999997,
      "loss": 1.3162,
      "step": 53
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.176073670387268,
      "learning_rate": 0.00019199999999999998,
      "loss": 1.1219,
      "step": 54
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.1324368715286255,
      "learning_rate": 0.00018999999999999998,
      "loss": 1.1305,
      "step": 55
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.096119999885559,
      "learning_rate": 0.000188,
      "loss": 1.3618,
      "step": 56
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.0667847394943237,
      "learning_rate": 0.000186,
      "loss": 1.3649,
      "step": 57
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.0742892026901245,
      "learning_rate": 0.00018399999999999997,
      "loss": 1.3841,
      "step": 58
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.1375027894973755,
      "learning_rate": 0.00018199999999999998,
      "loss": 1.3511,
      "step": 59
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.1602210998535156,
      "learning_rate": 0.00017999999999999998,
      "loss": 1.4042,
      "step": 60
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.2744908332824707,
      "learning_rate": 0.000178,
      "loss": 1.2273,
      "step": 61
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.2315484285354614,
      "learning_rate": 0.000176,
      "loss": 1.2758,
      "step": 62
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.2083710432052612,
      "learning_rate": 0.00017399999999999997,
      "loss": 1.2589,
      "step": 63
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.745052456855774,
      "learning_rate": 0.000172,
      "loss": 1.0773,
      "step": 64
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.702630877494812,
      "learning_rate": 0.00016999999999999999,
      "loss": 1.3124,
      "step": 65
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.4396088123321533,
      "learning_rate": 0.000168,
      "loss": 1.1384,
      "step": 66
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.4368433952331543,
      "learning_rate": 0.000166,
      "loss": 1.3062,
      "step": 67
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.2508870363235474,
      "learning_rate": 0.00016399999999999997,
      "loss": 1.3474,
      "step": 68
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.6386456489562988,
      "learning_rate": 0.000162,
      "loss": 1.3897,
      "step": 69
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.4230738878250122,
      "learning_rate": 0.00015999999999999999,
      "loss": 1.3821,
      "step": 70
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.4100922346115112,
      "learning_rate": 0.00015799999999999996,
      "loss": 1.125,
      "step": 71
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.4085850715637207,
      "learning_rate": 0.000156,
      "loss": 1.2081,
      "step": 72
    },
    {
      "epoch": 1.46,
      "grad_norm": 1.3631864786148071,
      "learning_rate": 0.00015399999999999998,
      "loss": 1.2676,
      "step": 73
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.4226144552230835,
      "learning_rate": 0.000152,
      "loss": 1.032,
      "step": 74
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.4170640707015991,
      "learning_rate": 0.00015,
      "loss": 1.2967,
      "step": 75
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.5473054647445679,
      "learning_rate": 0.000148,
      "loss": 1.5584,
      "step": 76
    },
    {
      "epoch": 1.54,
      "grad_norm": 1.539275050163269,
      "learning_rate": 0.000146,
      "loss": 1.3847,
      "step": 77
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.5940712690353394,
      "learning_rate": 0.00014399999999999998,
      "loss": 1.2144,
      "step": 78
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.2316229343414307,
      "learning_rate": 0.00014199999999999998,
      "loss": 1.4193,
      "step": 79
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.5433217287063599,
      "learning_rate": 0.00014,
      "loss": 1.2849,
      "step": 80
    },
    {
      "epoch": 1.62,
      "grad_norm": 1.5547280311584473,
      "learning_rate": 0.000138,
      "loss": 1.1286,
      "step": 81
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.6357922554016113,
      "learning_rate": 0.00013599999999999997,
      "loss": 1.457,
      "step": 82
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.5187233686447144,
      "learning_rate": 0.00013399999999999998,
      "loss": 1.1022,
      "step": 83
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.564031720161438,
      "learning_rate": 0.00013199999999999998,
      "loss": 1.2269,
      "step": 84
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.3639451265335083,
      "learning_rate": 0.00013,
      "loss": 1.2386,
      "step": 85
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.4667630195617676,
      "learning_rate": 0.000128,
      "loss": 1.2332,
      "step": 86
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.6159526109695435,
      "learning_rate": 0.00012599999999999997,
      "loss": 1.2359,
      "step": 87
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.3854038715362549,
      "learning_rate": 0.00012399999999999998,
      "loss": 1.3976,
      "step": 88
    },
    {
      "epoch": 1.78,
      "grad_norm": 1.5552220344543457,
      "learning_rate": 0.000122,
      "loss": 1.3821,
      "step": 89
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.718855857849121,
      "learning_rate": 0.00011999999999999999,
      "loss": 1.332,
      "step": 90
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.4357167482376099,
      "learning_rate": 0.00011799999999999998,
      "loss": 1.3716,
      "step": 91
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.5886136293411255,
      "learning_rate": 0.00011599999999999999,
      "loss": 1.2888,
      "step": 92
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1.3579779863357544,
      "learning_rate": 0.00011399999999999999,
      "loss": 1.4153,
      "step": 93
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.6239253282546997,
      "learning_rate": 0.000112,
      "loss": 1.407,
      "step": 94
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.980470895767212,
      "learning_rate": 0.00010999999999999998,
      "loss": 1.2873,
      "step": 95
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.7166484594345093,
      "learning_rate": 0.00010799999999999998,
      "loss": 1.4763,
      "step": 96
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.302185297012329,
      "learning_rate": 0.00010599999999999999,
      "loss": 1.4957,
      "step": 97
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.8180183172225952,
      "learning_rate": 0.000104,
      "loss": 1.3985,
      "step": 98
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.8862156867980957,
      "learning_rate": 0.000102,
      "loss": 0.9357,
      "step": 99
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.4817677736282349,
      "learning_rate": 9.999999999999999e-05,
      "loss": 1.3989,
      "step": 100
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.5299797058105469,
      "learning_rate": 9.799999999999998e-05,
      "loss": 1.133,
      "step": 101
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.5308579206466675,
      "learning_rate": 9.599999999999999e-05,
      "loss": 1.1657,
      "step": 102
    },
    {
      "epoch": 2.06,
      "grad_norm": 1.511001467704773,
      "learning_rate": 9.4e-05,
      "loss": 1.2587,
      "step": 103
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.3728077411651611,
      "learning_rate": 9.199999999999999e-05,
      "loss": 1.2398,
      "step": 104
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.3917077779769897,
      "learning_rate": 8.999999999999999e-05,
      "loss": 1.1859,
      "step": 105
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.7912142276763916,
      "learning_rate": 8.8e-05,
      "loss": 0.9744,
      "step": 106
    },
    {
      "epoch": 2.14,
      "grad_norm": 1.6817067861557007,
      "learning_rate": 8.6e-05,
      "loss": 1.2702,
      "step": 107
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.540589690208435,
      "learning_rate": 8.4e-05,
      "loss": 1.2136,
      "step": 108
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.3845465183258057,
      "learning_rate": 8.199999999999999e-05,
      "loss": 1.1455,
      "step": 109
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.8346145153045654,
      "learning_rate": 7.999999999999999e-05,
      "loss": 1.3124,
      "step": 110
    },
    {
      "epoch": 2.22,
      "grad_norm": 1.8625909090042114,
      "learning_rate": 7.8e-05,
      "loss": 0.9927,
      "step": 111
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.896916389465332,
      "learning_rate": 7.6e-05,
      "loss": 1.2152,
      "step": 112
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.506803274154663,
      "learning_rate": 7.4e-05,
      "loss": 1.1854,
      "step": 113
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.643236517906189,
      "learning_rate": 7.199999999999999e-05,
      "loss": 1.2295,
      "step": 114
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.611717700958252,
      "learning_rate": 7e-05,
      "loss": 1.2264,
      "step": 115
    },
    {
      "epoch": 2.32,
      "grad_norm": 2.0701029300689697,
      "learning_rate": 6.799999999999999e-05,
      "loss": 1.2171,
      "step": 116
    },
    {
      "epoch": 2.34,
      "grad_norm": 1.6691904067993164,
      "learning_rate": 6.599999999999999e-05,
      "loss": 1.2718,
      "step": 117
    },
    {
      "epoch": 2.36,
      "grad_norm": 2.123241901397705,
      "learning_rate": 6.4e-05,
      "loss": 1.2047,
      "step": 118
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.7421607971191406,
      "learning_rate": 6.199999999999999e-05,
      "loss": 1.2194,
      "step": 119
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.814742922782898,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 1.0574,
      "step": 120
    },
    {
      "epoch": 2.42,
      "grad_norm": 2.0321600437164307,
      "learning_rate": 5.7999999999999994e-05,
      "loss": 1.1931,
      "step": 121
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.5801244974136353,
      "learning_rate": 5.6e-05,
      "loss": 1.2588,
      "step": 122
    },
    {
      "epoch": 2.46,
      "grad_norm": 1.929476261138916,
      "learning_rate": 5.399999999999999e-05,
      "loss": 1.0884,
      "step": 123
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.8772894144058228,
      "learning_rate": 5.2e-05,
      "loss": 1.2652,
      "step": 124
    },
    {
      "epoch": 2.5,
      "grad_norm": 2.0687077045440674,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 1.2881,
      "step": 125
    },
    {
      "epoch": 2.52,
      "grad_norm": 1.4464077949523926,
      "learning_rate": 4.7999999999999994e-05,
      "loss": 1.2601,
      "step": 126
    },
    {
      "epoch": 2.54,
      "grad_norm": 1.6539645195007324,
      "learning_rate": 4.599999999999999e-05,
      "loss": 1.0821,
      "step": 127
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.7432538270950317,
      "learning_rate": 4.4e-05,
      "loss": 1.1792,
      "step": 128
    },
    {
      "epoch": 2.58,
      "grad_norm": 1.6143243312835693,
      "learning_rate": 4.2e-05,
      "loss": 1.1453,
      "step": 129
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.5942329168319702,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 1.1948,
      "step": 130
    },
    {
      "epoch": 2.62,
      "grad_norm": 1.9993077516555786,
      "learning_rate": 3.8e-05,
      "loss": 1.1655,
      "step": 131
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.934420108795166,
      "learning_rate": 3.5999999999999994e-05,
      "loss": 1.1231,
      "step": 132
    },
    {
      "epoch": 2.66,
      "grad_norm": 1.8756678104400635,
      "learning_rate": 3.399999999999999e-05,
      "loss": 0.9678,
      "step": 133
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.7047531604766846,
      "learning_rate": 3.2e-05,
      "loss": 1.1797,
      "step": 134
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.5881377458572388,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 1.2874,
      "step": 135
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.6499165296554565,
      "learning_rate": 2.8e-05,
      "loss": 1.3405,
      "step": 136
    },
    {
      "epoch": 2.74,
      "grad_norm": 1.589826226234436,
      "learning_rate": 2.6e-05,
      "loss": 1.2765,
      "step": 137
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.680782675743103,
      "learning_rate": 2.3999999999999997e-05,
      "loss": 1.3034,
      "step": 138
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 1.631185531616211,
      "learning_rate": 2.2e-05,
      "loss": 1.1738,
      "step": 139
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.712380290031433,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 1.2591,
      "step": 140
    },
    {
      "epoch": 2.82,
      "grad_norm": 1.819724678993225,
      "learning_rate": 1.7999999999999997e-05,
      "loss": 1.1066,
      "step": 141
    },
    {
      "epoch": 2.84,
      "grad_norm": 2.397151231765747,
      "learning_rate": 1.6e-05,
      "loss": 1.1764,
      "step": 142
    },
    {
      "epoch": 2.86,
      "grad_norm": 1.5550163984298706,
      "learning_rate": 1.4e-05,
      "loss": 1.067,
      "step": 143
    },
    {
      "epoch": 2.88,
      "grad_norm": 2.0756919384002686,
      "learning_rate": 1.1999999999999999e-05,
      "loss": 1.2877,
      "step": 144
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.6905994415283203,
      "learning_rate": 9.999999999999999e-06,
      "loss": 1.2048,
      "step": 145
    },
    {
      "epoch": 2.92,
      "grad_norm": 1.4660894870758057,
      "learning_rate": 8e-06,
      "loss": 1.2071,
      "step": 146
    },
    {
      "epoch": 2.94,
      "grad_norm": 1.9286458492279053,
      "learning_rate": 5.999999999999999e-06,
      "loss": 1.357,
      "step": 147
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.8045306205749512,
      "learning_rate": 4e-06,
      "loss": 1.1121,
      "step": 148
    },
    {
      "epoch": 2.98,
      "grad_norm": 1.3637385368347168,
      "learning_rate": 2e-06,
      "loss": 1.255,
      "step": 149
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.9857360124588013,
      "learning_rate": 0.0,
      "loss": 1.0083,
      "step": 150
    }
  ],
  "logging_steps": 1,
  "max_steps": 150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1091669233958912e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}

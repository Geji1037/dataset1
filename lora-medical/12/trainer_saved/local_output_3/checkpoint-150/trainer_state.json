{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 0.6241187453269958,
      "learning_rate": 0.000298,
      "loss": 1.627,
      "step": 1
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.537600576877594,
      "learning_rate": 0.000296,
      "loss": 1.6369,
      "step": 2
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.6474688053131104,
      "learning_rate": 0.000294,
      "loss": 1.7613,
      "step": 3
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.7678049802780151,
      "learning_rate": 0.000292,
      "loss": 1.7407,
      "step": 4
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.2006535530090332,
      "learning_rate": 0.00029,
      "loss": 1.6589,
      "step": 5
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.8168783187866211,
      "learning_rate": 0.00028799999999999995,
      "loss": 1.5297,
      "step": 6
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.8572777509689331,
      "learning_rate": 0.00028599999999999996,
      "loss": 1.5189,
      "step": 7
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.4612178802490234,
      "learning_rate": 0.00028399999999999996,
      "loss": 1.6143,
      "step": 8
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.713773250579834,
      "learning_rate": 0.00028199999999999997,
      "loss": 1.7341,
      "step": 9
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.3269081115722656,
      "learning_rate": 0.00028,
      "loss": 1.4372,
      "step": 10
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.3481109142303467,
      "learning_rate": 0.000278,
      "loss": 1.6451,
      "step": 11
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.3864167928695679,
      "learning_rate": 0.000276,
      "loss": 1.6348,
      "step": 12
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.154313087463379,
      "learning_rate": 0.000274,
      "loss": 1.5468,
      "step": 13
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.3162624835968018,
      "learning_rate": 0.00027199999999999994,
      "loss": 1.4633,
      "step": 14
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.1694366931915283,
      "learning_rate": 0.00027,
      "loss": 1.3557,
      "step": 15
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.4566689729690552,
      "learning_rate": 0.00026799999999999995,
      "loss": 1.2638,
      "step": 16
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.2005733251571655,
      "learning_rate": 0.000266,
      "loss": 1.4924,
      "step": 17
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.5726374387741089,
      "learning_rate": 0.00026399999999999997,
      "loss": 1.3462,
      "step": 18
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.3284543752670288,
      "learning_rate": 0.00026199999999999997,
      "loss": 1.481,
      "step": 19
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.378407597541809,
      "learning_rate": 0.00026,
      "loss": 1.6268,
      "step": 20
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.3372095823287964,
      "learning_rate": 0.000258,
      "loss": 1.3956,
      "step": 21
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.2506057024002075,
      "learning_rate": 0.000256,
      "loss": 1.6097,
      "step": 22
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.082973837852478,
      "learning_rate": 0.000254,
      "loss": 1.3791,
      "step": 23
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.4112403392791748,
      "learning_rate": 0.00025199999999999995,
      "loss": 1.3746,
      "step": 24
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.187282681465149,
      "learning_rate": 0.00025,
      "loss": 1.3859,
      "step": 25
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.2652158737182617,
      "learning_rate": 0.00024799999999999996,
      "loss": 1.364,
      "step": 26
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.1840546131134033,
      "learning_rate": 0.00024599999999999996,
      "loss": 1.394,
      "step": 27
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.1523993015289307,
      "learning_rate": 0.000244,
      "loss": 1.4112,
      "step": 28
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.4199813604354858,
      "learning_rate": 0.00024199999999999997,
      "loss": 1.32,
      "step": 29
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.1032514572143555,
      "learning_rate": 0.00023999999999999998,
      "loss": 1.0155,
      "step": 30
    },
    {
      "epoch": 0.62,
      "grad_norm": 1.0436878204345703,
      "learning_rate": 0.00023799999999999998,
      "loss": 1.4072,
      "step": 31
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.4533404111862183,
      "learning_rate": 0.00023599999999999996,
      "loss": 1.3295,
      "step": 32
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.240371584892273,
      "learning_rate": 0.000234,
      "loss": 1.3683,
      "step": 33
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.4425084590911865,
      "learning_rate": 0.00023199999999999997,
      "loss": 1.409,
      "step": 34
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.2605907917022705,
      "learning_rate": 0.00023,
      "loss": 1.3551,
      "step": 35
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.2038302421569824,
      "learning_rate": 0.00022799999999999999,
      "loss": 1.3204,
      "step": 36
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.1804875135421753,
      "learning_rate": 0.00022599999999999996,
      "loss": 1.5652,
      "step": 37
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.0539953708648682,
      "learning_rate": 0.000224,
      "loss": 1.3168,
      "step": 38
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.136113166809082,
      "learning_rate": 0.00022199999999999998,
      "loss": 1.4187,
      "step": 39
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.3387748003005981,
      "learning_rate": 0.00021999999999999995,
      "loss": 1.3515,
      "step": 40
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.106755018234253,
      "learning_rate": 0.00021799999999999999,
      "loss": 1.3829,
      "step": 41
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.0062459707260132,
      "learning_rate": 0.00021599999999999996,
      "loss": 1.4463,
      "step": 42
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.2086331844329834,
      "learning_rate": 0.000214,
      "loss": 1.345,
      "step": 43
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.9926283359527588,
      "learning_rate": 0.00021199999999999998,
      "loss": 1.547,
      "step": 44
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.0762826204299927,
      "learning_rate": 0.00020999999999999998,
      "loss": 1.2507,
      "step": 45
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.078609824180603,
      "learning_rate": 0.000208,
      "loss": 1.2547,
      "step": 46
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.0734996795654297,
      "learning_rate": 0.00020599999999999997,
      "loss": 1.4953,
      "step": 47
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.5600134134292603,
      "learning_rate": 0.000204,
      "loss": 1.3168,
      "step": 48
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.2237119674682617,
      "learning_rate": 0.00020199999999999998,
      "loss": 1.2017,
      "step": 49
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.145936369895935,
      "learning_rate": 0.00019999999999999998,
      "loss": 1.575,
      "step": 50
    },
    {
      "epoch": 1.02,
      "grad_norm": 1.083940863609314,
      "learning_rate": 0.000198,
      "loss": 1.2318,
      "step": 51
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.1461950540542603,
      "learning_rate": 0.00019599999999999997,
      "loss": 1.3005,
      "step": 52
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.316561222076416,
      "learning_rate": 0.00019399999999999997,
      "loss": 1.4502,
      "step": 53
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.344313383102417,
      "learning_rate": 0.00019199999999999998,
      "loss": 1.2312,
      "step": 54
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.3685803413391113,
      "learning_rate": 0.00018999999999999998,
      "loss": 1.1116,
      "step": 55
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.2457220554351807,
      "learning_rate": 0.000188,
      "loss": 1.2904,
      "step": 56
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.466144323348999,
      "learning_rate": 0.000186,
      "loss": 1.2761,
      "step": 57
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.543397068977356,
      "learning_rate": 0.00018399999999999997,
      "loss": 1.3618,
      "step": 58
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.3516331911087036,
      "learning_rate": 0.00018199999999999998,
      "loss": 1.3706,
      "step": 59
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.5077049732208252,
      "learning_rate": 0.00017999999999999998,
      "loss": 1.3823,
      "step": 60
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.1028739213943481,
      "learning_rate": 0.000178,
      "loss": 1.2534,
      "step": 61
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.4137542247772217,
      "learning_rate": 0.000176,
      "loss": 1.0658,
      "step": 62
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.4421719312667847,
      "learning_rate": 0.00017399999999999997,
      "loss": 1.3603,
      "step": 63
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.494382619857788,
      "learning_rate": 0.000172,
      "loss": 1.1662,
      "step": 64
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.4582563638687134,
      "learning_rate": 0.00016999999999999999,
      "loss": 1.3542,
      "step": 65
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.4799084663391113,
      "learning_rate": 0.000168,
      "loss": 1.2925,
      "step": 66
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.4191875457763672,
      "learning_rate": 0.000166,
      "loss": 1.182,
      "step": 67
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.4887720346450806,
      "learning_rate": 0.00016399999999999997,
      "loss": 1.1371,
      "step": 68
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.7975776195526123,
      "learning_rate": 0.000162,
      "loss": 1.414,
      "step": 69
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.6246378421783447,
      "learning_rate": 0.00015999999999999999,
      "loss": 1.2221,
      "step": 70
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.639163613319397,
      "learning_rate": 0.00015799999999999996,
      "loss": 1.3173,
      "step": 71
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.634549856185913,
      "learning_rate": 0.000156,
      "loss": 1.138,
      "step": 72
    },
    {
      "epoch": 1.46,
      "grad_norm": 1.3788217306137085,
      "learning_rate": 0.00015399999999999998,
      "loss": 1.1263,
      "step": 73
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.5023223161697388,
      "learning_rate": 0.000152,
      "loss": 1.2358,
      "step": 74
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.3322182893753052,
      "learning_rate": 0.00015,
      "loss": 1.1694,
      "step": 75
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.3637880086898804,
      "learning_rate": 0.000148,
      "loss": 1.0434,
      "step": 76
    },
    {
      "epoch": 1.54,
      "grad_norm": 1.6817013025283813,
      "learning_rate": 0.000146,
      "loss": 1.0206,
      "step": 77
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.6631150245666504,
      "learning_rate": 0.00014399999999999998,
      "loss": 1.0667,
      "step": 78
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.504693865776062,
      "learning_rate": 0.00014199999999999998,
      "loss": 1.1127,
      "step": 79
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.6049704551696777,
      "learning_rate": 0.00014,
      "loss": 1.4384,
      "step": 80
    },
    {
      "epoch": 1.62,
      "grad_norm": 1.5976988077163696,
      "learning_rate": 0.000138,
      "loss": 1.187,
      "step": 81
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.2758013010025024,
      "learning_rate": 0.00013599999999999997,
      "loss": 1.337,
      "step": 82
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.7124475240707397,
      "learning_rate": 0.00013399999999999998,
      "loss": 1.2883,
      "step": 83
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.7701570987701416,
      "learning_rate": 0.00013199999999999998,
      "loss": 1.1322,
      "step": 84
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.5772250890731812,
      "learning_rate": 0.00013,
      "loss": 0.8988,
      "step": 85
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.5060670375823975,
      "learning_rate": 0.000128,
      "loss": 1.2578,
      "step": 86
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.7136647701263428,
      "learning_rate": 0.00012599999999999997,
      "loss": 1.1638,
      "step": 87
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.62410569190979,
      "learning_rate": 0.00012399999999999998,
      "loss": 1.2698,
      "step": 88
    },
    {
      "epoch": 1.78,
      "grad_norm": 1.5895702838897705,
      "learning_rate": 0.000122,
      "loss": 1.2821,
      "step": 89
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.5104697942733765,
      "learning_rate": 0.00011999999999999999,
      "loss": 1.2227,
      "step": 90
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.3977394104003906,
      "learning_rate": 0.00011799999999999998,
      "loss": 1.2048,
      "step": 91
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.4713404178619385,
      "learning_rate": 0.00011599999999999999,
      "loss": 1.0922,
      "step": 92
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1.367031455039978,
      "learning_rate": 0.00011399999999999999,
      "loss": 1.2277,
      "step": 93
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.529677152633667,
      "learning_rate": 0.000112,
      "loss": 1.0064,
      "step": 94
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.403024673461914,
      "learning_rate": 0.00010999999999999998,
      "loss": 1.2567,
      "step": 95
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.6272475719451904,
      "learning_rate": 0.00010799999999999998,
      "loss": 1.4353,
      "step": 96
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.8728749752044678,
      "learning_rate": 0.00010599999999999999,
      "loss": 1.3028,
      "step": 97
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.5139566659927368,
      "learning_rate": 0.000104,
      "loss": 1.3195,
      "step": 98
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.5449609756469727,
      "learning_rate": 0.000102,
      "loss": 1.0206,
      "step": 99
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.013883590698242,
      "learning_rate": 9.999999999999999e-05,
      "loss": 1.1958,
      "step": 100
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.6078099012374878,
      "learning_rate": 9.799999999999998e-05,
      "loss": 1.0215,
      "step": 101
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.3128993511199951,
      "learning_rate": 9.599999999999999e-05,
      "loss": 1.3563,
      "step": 102
    },
    {
      "epoch": 2.06,
      "grad_norm": 1.5044199228286743,
      "learning_rate": 9.4e-05,
      "loss": 1.1864,
      "step": 103
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.5126712322235107,
      "learning_rate": 9.199999999999999e-05,
      "loss": 0.9427,
      "step": 104
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.700934886932373,
      "learning_rate": 8.999999999999999e-05,
      "loss": 1.3078,
      "step": 105
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.3823658227920532,
      "learning_rate": 8.8e-05,
      "loss": 1.0921,
      "step": 106
    },
    {
      "epoch": 2.14,
      "grad_norm": 1.4062830209732056,
      "learning_rate": 8.6e-05,
      "loss": 1.103,
      "step": 107
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.4459360837936401,
      "learning_rate": 8.4e-05,
      "loss": 1.0717,
      "step": 108
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.5444858074188232,
      "learning_rate": 8.199999999999999e-05,
      "loss": 1.1252,
      "step": 109
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.826843023300171,
      "learning_rate": 7.999999999999999e-05,
      "loss": 1.1411,
      "step": 110
    },
    {
      "epoch": 2.22,
      "grad_norm": 1.7840042114257812,
      "learning_rate": 7.8e-05,
      "loss": 1.1917,
      "step": 111
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.6842927932739258,
      "learning_rate": 7.6e-05,
      "loss": 1.0362,
      "step": 112
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.5689345598220825,
      "learning_rate": 7.4e-05,
      "loss": 1.3088,
      "step": 113
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.5629422664642334,
      "learning_rate": 7.199999999999999e-05,
      "loss": 1.0526,
      "step": 114
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.6484723091125488,
      "learning_rate": 7e-05,
      "loss": 1.3908,
      "step": 115
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.7505974769592285,
      "learning_rate": 6.799999999999999e-05,
      "loss": 1.0317,
      "step": 116
    },
    {
      "epoch": 2.34,
      "grad_norm": 1.5838782787322998,
      "learning_rate": 6.599999999999999e-05,
      "loss": 1.0249,
      "step": 117
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.881282091140747,
      "learning_rate": 6.4e-05,
      "loss": 1.1139,
      "step": 118
    },
    {
      "epoch": 2.38,
      "grad_norm": 2.0120503902435303,
      "learning_rate": 6.199999999999999e-05,
      "loss": 1.0649,
      "step": 119
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.8668087720870972,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 0.9497,
      "step": 120
    },
    {
      "epoch": 2.42,
      "grad_norm": 1.6999478340148926,
      "learning_rate": 5.7999999999999994e-05,
      "loss": 0.9621,
      "step": 121
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.7843613624572754,
      "learning_rate": 5.6e-05,
      "loss": 1.0306,
      "step": 122
    },
    {
      "epoch": 2.46,
      "grad_norm": 1.6595299243927002,
      "learning_rate": 5.399999999999999e-05,
      "loss": 1.1207,
      "step": 123
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.735744833946228,
      "learning_rate": 5.2e-05,
      "loss": 1.0794,
      "step": 124
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.866280198097229,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 1.0931,
      "step": 125
    },
    {
      "epoch": 2.52,
      "grad_norm": 2.3377459049224854,
      "learning_rate": 4.7999999999999994e-05,
      "loss": 1.0222,
      "step": 126
    },
    {
      "epoch": 2.54,
      "grad_norm": 1.8628188371658325,
      "learning_rate": 4.599999999999999e-05,
      "loss": 1.1068,
      "step": 127
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.5186688899993896,
      "learning_rate": 4.4e-05,
      "loss": 1.2289,
      "step": 128
    },
    {
      "epoch": 2.58,
      "grad_norm": 2.0954482555389404,
      "learning_rate": 4.2e-05,
      "loss": 1.0108,
      "step": 129
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.9333856105804443,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 1.1138,
      "step": 130
    },
    {
      "epoch": 2.62,
      "grad_norm": 1.8192893266677856,
      "learning_rate": 3.8e-05,
      "loss": 1.1167,
      "step": 131
    },
    {
      "epoch": 2.64,
      "grad_norm": 2.027665376663208,
      "learning_rate": 3.5999999999999994e-05,
      "loss": 1.3043,
      "step": 132
    },
    {
      "epoch": 2.66,
      "grad_norm": 2.2523410320281982,
      "learning_rate": 3.399999999999999e-05,
      "loss": 1.1947,
      "step": 133
    },
    {
      "epoch": 2.68,
      "grad_norm": 2.376868486404419,
      "learning_rate": 3.2e-05,
      "loss": 0.9644,
      "step": 134
    },
    {
      "epoch": 2.7,
      "grad_norm": 2.02268385887146,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 0.9618,
      "step": 135
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 2.104823589324951,
      "learning_rate": 2.8e-05,
      "loss": 1.1184,
      "step": 136
    },
    {
      "epoch": 2.74,
      "grad_norm": 1.7035516500473022,
      "learning_rate": 2.6e-05,
      "loss": 1.2976,
      "step": 137
    },
    {
      "epoch": 2.76,
      "grad_norm": 2.0956881046295166,
      "learning_rate": 2.3999999999999997e-05,
      "loss": 1.0472,
      "step": 138
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 1.6265556812286377,
      "learning_rate": 2.2e-05,
      "loss": 0.9927,
      "step": 139
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.5712679624557495,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 1.0505,
      "step": 140
    },
    {
      "epoch": 2.82,
      "grad_norm": 2.1068828105926514,
      "learning_rate": 1.7999999999999997e-05,
      "loss": 0.883,
      "step": 141
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.6053040027618408,
      "learning_rate": 1.6e-05,
      "loss": 1.102,
      "step": 142
    },
    {
      "epoch": 2.86,
      "grad_norm": 1.9008640050888062,
      "learning_rate": 1.4e-05,
      "loss": 1.0861,
      "step": 143
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.8001738786697388,
      "learning_rate": 1.1999999999999999e-05,
      "loss": 1.169,
      "step": 144
    },
    {
      "epoch": 2.9,
      "grad_norm": 2.2095892429351807,
      "learning_rate": 9.999999999999999e-06,
      "loss": 1.2116,
      "step": 145
    },
    {
      "epoch": 2.92,
      "grad_norm": 2.0451340675354004,
      "learning_rate": 8e-06,
      "loss": 1.0401,
      "step": 146
    },
    {
      "epoch": 2.94,
      "grad_norm": 2.4850850105285645,
      "learning_rate": 5.999999999999999e-06,
      "loss": 0.8166,
      "step": 147
    },
    {
      "epoch": 2.96,
      "grad_norm": 2.0892841815948486,
      "learning_rate": 4e-06,
      "loss": 1.1801,
      "step": 148
    },
    {
      "epoch": 2.98,
      "grad_norm": 1.8701092004776,
      "learning_rate": 2e-06,
      "loss": 1.06,
      "step": 149
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.973198413848877,
      "learning_rate": 0.0,
      "loss": 1.0494,
      "step": 150
    }
  ],
  "logging_steps": 1,
  "max_steps": 150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1548656385916928e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}

{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 0.5915420651435852,
      "learning_rate": 0.000298,
      "loss": 1.8319,
      "step": 1
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.6990802884101868,
      "learning_rate": 0.000296,
      "loss": 1.9264,
      "step": 2
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.7976139783859253,
      "learning_rate": 0.000294,
      "loss": 1.7859,
      "step": 3
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.895790696144104,
      "learning_rate": 0.000292,
      "loss": 1.9208,
      "step": 4
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.0544673204421997,
      "learning_rate": 0.00029,
      "loss": 1.6789,
      "step": 5
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.8574718236923218,
      "learning_rate": 0.00028799999999999995,
      "loss": 1.6712,
      "step": 6
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.8281434774398804,
      "learning_rate": 0.00028599999999999996,
      "loss": 1.4102,
      "step": 7
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.6205047369003296,
      "learning_rate": 0.00028399999999999996,
      "loss": 1.6312,
      "step": 8
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.528584361076355,
      "learning_rate": 0.00028199999999999997,
      "loss": 1.8198,
      "step": 9
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.360791563987732,
      "learning_rate": 0.00028,
      "loss": 1.7402,
      "step": 10
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.1802575588226318,
      "learning_rate": 0.000278,
      "loss": 1.6462,
      "step": 11
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.364001989364624,
      "learning_rate": 0.000276,
      "loss": 1.5844,
      "step": 12
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.0759365558624268,
      "learning_rate": 0.000274,
      "loss": 1.5874,
      "step": 13
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.9727500677108765,
      "learning_rate": 0.00027199999999999994,
      "loss": 1.6206,
      "step": 14
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.1041868925094604,
      "learning_rate": 0.00027,
      "loss": 1.4711,
      "step": 15
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.1152722835540771,
      "learning_rate": 0.00026799999999999995,
      "loss": 1.6928,
      "step": 16
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.2211859226226807,
      "learning_rate": 0.000266,
      "loss": 1.3409,
      "step": 17
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.1795411109924316,
      "learning_rate": 0.00026399999999999997,
      "loss": 1.5474,
      "step": 18
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.2100188732147217,
      "learning_rate": 0.00026199999999999997,
      "loss": 1.3761,
      "step": 19
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.413648009300232,
      "learning_rate": 0.00026,
      "loss": 1.5794,
      "step": 20
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.2221602201461792,
      "learning_rate": 0.000258,
      "loss": 1.3981,
      "step": 21
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.2258504629135132,
      "learning_rate": 0.000256,
      "loss": 1.4418,
      "step": 22
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.2010486125946045,
      "learning_rate": 0.000254,
      "loss": 1.4758,
      "step": 23
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.1574833393096924,
      "learning_rate": 0.00025199999999999995,
      "loss": 1.5612,
      "step": 24
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.2788968086242676,
      "learning_rate": 0.00025,
      "loss": 1.5384,
      "step": 25
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.1743896007537842,
      "learning_rate": 0.00024799999999999996,
      "loss": 1.3822,
      "step": 26
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.2015137672424316,
      "learning_rate": 0.00024599999999999996,
      "loss": 1.4884,
      "step": 27
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.0808861255645752,
      "learning_rate": 0.000244,
      "loss": 1.2857,
      "step": 28
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.8873819708824158,
      "learning_rate": 0.00024199999999999997,
      "loss": 1.4926,
      "step": 29
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.352062702178955,
      "learning_rate": 0.00023999999999999998,
      "loss": 1.3892,
      "step": 30
    },
    {
      "epoch": 0.62,
      "grad_norm": 1.2349947690963745,
      "learning_rate": 0.00023799999999999998,
      "loss": 1.3841,
      "step": 31
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.0772255659103394,
      "learning_rate": 0.00023599999999999996,
      "loss": 1.427,
      "step": 32
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.0246928930282593,
      "learning_rate": 0.000234,
      "loss": 1.2553,
      "step": 33
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.128966212272644,
      "learning_rate": 0.00023199999999999997,
      "loss": 1.3376,
      "step": 34
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.9998719692230225,
      "learning_rate": 0.00023,
      "loss": 1.5044,
      "step": 35
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.5388166904449463,
      "learning_rate": 0.00022799999999999999,
      "loss": 1.3575,
      "step": 36
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.9507395029067993,
      "learning_rate": 0.00022599999999999996,
      "loss": 1.4941,
      "step": 37
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.085035800933838,
      "learning_rate": 0.000224,
      "loss": 1.486,
      "step": 38
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.2107834815979004,
      "learning_rate": 0.00022199999999999998,
      "loss": 1.5456,
      "step": 39
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.1136674880981445,
      "learning_rate": 0.00021999999999999995,
      "loss": 1.3825,
      "step": 40
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.9684261679649353,
      "learning_rate": 0.00021799999999999999,
      "loss": 1.5698,
      "step": 41
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.1090445518493652,
      "learning_rate": 0.00021599999999999996,
      "loss": 1.3371,
      "step": 42
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.2930079698562622,
      "learning_rate": 0.000214,
      "loss": 1.4385,
      "step": 43
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.2322789430618286,
      "learning_rate": 0.00021199999999999998,
      "loss": 1.506,
      "step": 44
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.4302681684494019,
      "learning_rate": 0.00020999999999999998,
      "loss": 1.3698,
      "step": 45
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.8582966923713684,
      "learning_rate": 0.000208,
      "loss": 1.5597,
      "step": 46
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.274901270866394,
      "learning_rate": 0.00020599999999999997,
      "loss": 1.291,
      "step": 47
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.2123230695724487,
      "learning_rate": 0.000204,
      "loss": 1.5974,
      "step": 48
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.0015748739242554,
      "learning_rate": 0.00020199999999999998,
      "loss": 1.5206,
      "step": 49
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.4062288999557495,
      "learning_rate": 0.00019999999999999998,
      "loss": 1.5232,
      "step": 50
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.8996962904930115,
      "learning_rate": 0.000198,
      "loss": 1.4516,
      "step": 51
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.2685238122940063,
      "learning_rate": 0.00019599999999999997,
      "loss": 1.497,
      "step": 52
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.2365182638168335,
      "learning_rate": 0.00019399999999999997,
      "loss": 1.3023,
      "step": 53
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.939887523651123,
      "learning_rate": 0.00019199999999999998,
      "loss": 1.3636,
      "step": 54
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.0665512084960938,
      "learning_rate": 0.00018999999999999998,
      "loss": 1.1796,
      "step": 55
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.0825906991958618,
      "learning_rate": 0.000188,
      "loss": 1.257,
      "step": 56
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.1311252117156982,
      "learning_rate": 0.000186,
      "loss": 1.3985,
      "step": 57
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.0342594385147095,
      "learning_rate": 0.00018399999999999997,
      "loss": 1.297,
      "step": 58
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.430406093597412,
      "learning_rate": 0.00018199999999999998,
      "loss": 1.2282,
      "step": 59
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.3133749961853027,
      "learning_rate": 0.00017999999999999998,
      "loss": 1.062,
      "step": 60
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.2034741640090942,
      "learning_rate": 0.000178,
      "loss": 1.3913,
      "step": 61
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.2939302921295166,
      "learning_rate": 0.000176,
      "loss": 1.2613,
      "step": 62
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.1066226959228516,
      "learning_rate": 0.00017399999999999997,
      "loss": 1.3969,
      "step": 63
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.282468318939209,
      "learning_rate": 0.000172,
      "loss": 1.0,
      "step": 64
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.2629963159561157,
      "learning_rate": 0.00016999999999999999,
      "loss": 1.4265,
      "step": 65
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.3098915815353394,
      "learning_rate": 0.000168,
      "loss": 1.247,
      "step": 66
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.3337312936782837,
      "learning_rate": 0.000166,
      "loss": 1.2506,
      "step": 67
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.1516731977462769,
      "learning_rate": 0.00016399999999999997,
      "loss": 1.2681,
      "step": 68
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.6967037916183472,
      "learning_rate": 0.000162,
      "loss": 1.3771,
      "step": 69
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.5545059442520142,
      "learning_rate": 0.00015999999999999999,
      "loss": 1.3702,
      "step": 70
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.220918893814087,
      "learning_rate": 0.00015799999999999996,
      "loss": 1.1917,
      "step": 71
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.4216370582580566,
      "learning_rate": 0.000156,
      "loss": 1.2115,
      "step": 72
    },
    {
      "epoch": 1.46,
      "grad_norm": 1.1575764417648315,
      "learning_rate": 0.00015399999999999998,
      "loss": 1.3946,
      "step": 73
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.143271803855896,
      "learning_rate": 0.000152,
      "loss": 1.4033,
      "step": 74
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.3347296714782715,
      "learning_rate": 0.00015,
      "loss": 1.3124,
      "step": 75
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.4130703210830688,
      "learning_rate": 0.000148,
      "loss": 1.4508,
      "step": 76
    },
    {
      "epoch": 1.54,
      "grad_norm": 1.2913607358932495,
      "learning_rate": 0.000146,
      "loss": 1.2807,
      "step": 77
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.542109727859497,
      "learning_rate": 0.00014399999999999998,
      "loss": 1.3305,
      "step": 78
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.383010983467102,
      "learning_rate": 0.00014199999999999998,
      "loss": 1.3733,
      "step": 79
    },
    {
      "epoch": 1.6,
      "grad_norm": 2.2036495208740234,
      "learning_rate": 0.00014,
      "loss": 1.3224,
      "step": 80
    },
    {
      "epoch": 1.62,
      "grad_norm": 1.3018275499343872,
      "learning_rate": 0.000138,
      "loss": 1.1564,
      "step": 81
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 2.0114657878875732,
      "learning_rate": 0.00013599999999999997,
      "loss": 1.1261,
      "step": 82
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.5132689476013184,
      "learning_rate": 0.00013399999999999998,
      "loss": 1.4565,
      "step": 83
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.3320752382278442,
      "learning_rate": 0.00013199999999999998,
      "loss": 1.3893,
      "step": 84
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.4737811088562012,
      "learning_rate": 0.00013,
      "loss": 1.3409,
      "step": 85
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.3564238548278809,
      "learning_rate": 0.000128,
      "loss": 1.2162,
      "step": 86
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.9698034524917603,
      "learning_rate": 0.00012599999999999997,
      "loss": 1.3805,
      "step": 87
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.3830888271331787,
      "learning_rate": 0.00012399999999999998,
      "loss": 1.3237,
      "step": 88
    },
    {
      "epoch": 1.78,
      "grad_norm": 1.4741597175598145,
      "learning_rate": 0.000122,
      "loss": 1.4293,
      "step": 89
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.686972975730896,
      "learning_rate": 0.00011999999999999999,
      "loss": 1.3365,
      "step": 90
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.8302066326141357,
      "learning_rate": 0.00011799999999999998,
      "loss": 1.438,
      "step": 91
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.4452084302902222,
      "learning_rate": 0.00011599999999999999,
      "loss": 1.354,
      "step": 92
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1.6091481447219849,
      "learning_rate": 0.00011399999999999999,
      "loss": 1.3406,
      "step": 93
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.825972557067871,
      "learning_rate": 0.000112,
      "loss": 1.3957,
      "step": 94
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.6980915069580078,
      "learning_rate": 0.00010999999999999998,
      "loss": 1.2736,
      "step": 95
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.6841487884521484,
      "learning_rate": 0.00010799999999999998,
      "loss": 1.2757,
      "step": 96
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.990173101425171,
      "learning_rate": 0.00010599999999999999,
      "loss": 1.2142,
      "step": 97
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.5174882411956787,
      "learning_rate": 0.000104,
      "loss": 1.5959,
      "step": 98
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.3301048278808594,
      "learning_rate": 0.000102,
      "loss": 1.2942,
      "step": 99
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.6369688510894775,
      "learning_rate": 9.999999999999999e-05,
      "loss": 1.3979,
      "step": 100
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.4320958852767944,
      "learning_rate": 9.799999999999998e-05,
      "loss": 1.3319,
      "step": 101
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.9009087085723877,
      "learning_rate": 9.599999999999999e-05,
      "loss": 1.18,
      "step": 102
    },
    {
      "epoch": 2.06,
      "grad_norm": 1.429118037223816,
      "learning_rate": 9.4e-05,
      "loss": 1.1908,
      "step": 103
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.6333332061767578,
      "learning_rate": 9.199999999999999e-05,
      "loss": 1.3344,
      "step": 104
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.1894621849060059,
      "learning_rate": 8.999999999999999e-05,
      "loss": 1.177,
      "step": 105
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.5697665214538574,
      "learning_rate": 8.8e-05,
      "loss": 1.3955,
      "step": 106
    },
    {
      "epoch": 2.14,
      "grad_norm": 1.3308073282241821,
      "learning_rate": 8.6e-05,
      "loss": 0.8819,
      "step": 107
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.791516900062561,
      "learning_rate": 8.4e-05,
      "loss": 1.2939,
      "step": 108
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.31955087184906,
      "learning_rate": 8.199999999999999e-05,
      "loss": 1.1326,
      "step": 109
    },
    {
      "epoch": 2.2,
      "grad_norm": 2.046377658843994,
      "learning_rate": 7.999999999999999e-05,
      "loss": 1.2211,
      "step": 110
    },
    {
      "epoch": 2.22,
      "grad_norm": 1.3476375341415405,
      "learning_rate": 7.8e-05,
      "loss": 1.1392,
      "step": 111
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.8048192262649536,
      "learning_rate": 7.6e-05,
      "loss": 1.3454,
      "step": 112
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.7686519622802734,
      "learning_rate": 7.4e-05,
      "loss": 1.1632,
      "step": 113
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.9135020971298218,
      "learning_rate": 7.199999999999999e-05,
      "loss": 0.9036,
      "step": 114
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.3244032859802246,
      "learning_rate": 7e-05,
      "loss": 1.3998,
      "step": 115
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.594325065612793,
      "learning_rate": 6.799999999999999e-05,
      "loss": 1.2735,
      "step": 116
    },
    {
      "epoch": 2.34,
      "grad_norm": 1.8137640953063965,
      "learning_rate": 6.599999999999999e-05,
      "loss": 1.1546,
      "step": 117
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.537137746810913,
      "learning_rate": 6.4e-05,
      "loss": 1.3654,
      "step": 118
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.568665862083435,
      "learning_rate": 6.199999999999999e-05,
      "loss": 1.1499,
      "step": 119
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.6205569505691528,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 1.3207,
      "step": 120
    },
    {
      "epoch": 2.42,
      "grad_norm": 1.8337085247039795,
      "learning_rate": 5.7999999999999994e-05,
      "loss": 1.0717,
      "step": 121
    },
    {
      "epoch": 2.44,
      "grad_norm": 2.207225799560547,
      "learning_rate": 5.6e-05,
      "loss": 1.249,
      "step": 122
    },
    {
      "epoch": 2.46,
      "grad_norm": 1.8157950639724731,
      "learning_rate": 5.399999999999999e-05,
      "loss": 1.2872,
      "step": 123
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.7987687587738037,
      "learning_rate": 5.2e-05,
      "loss": 1.0374,
      "step": 124
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.5797998905181885,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 1.1669,
      "step": 125
    },
    {
      "epoch": 2.52,
      "grad_norm": 1.803653359413147,
      "learning_rate": 4.7999999999999994e-05,
      "loss": 1.2353,
      "step": 126
    },
    {
      "epoch": 2.54,
      "grad_norm": 1.7565863132476807,
      "learning_rate": 4.599999999999999e-05,
      "loss": 1.3053,
      "step": 127
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.701064109802246,
      "learning_rate": 4.4e-05,
      "loss": 1.3714,
      "step": 128
    },
    {
      "epoch": 2.58,
      "grad_norm": 1.3411142826080322,
      "learning_rate": 4.2e-05,
      "loss": 1.1873,
      "step": 129
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.6369376182556152,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 1.2866,
      "step": 130
    },
    {
      "epoch": 2.62,
      "grad_norm": 1.6278365850448608,
      "learning_rate": 3.8e-05,
      "loss": 1.2916,
      "step": 131
    },
    {
      "epoch": 2.64,
      "grad_norm": 2.0020954608917236,
      "learning_rate": 3.5999999999999994e-05,
      "loss": 1.1492,
      "step": 132
    },
    {
      "epoch": 2.66,
      "grad_norm": 1.5325654745101929,
      "learning_rate": 3.399999999999999e-05,
      "loss": 1.3005,
      "step": 133
    },
    {
      "epoch": 2.68,
      "grad_norm": 2.1861507892608643,
      "learning_rate": 3.2e-05,
      "loss": 1.3181,
      "step": 134
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.525210976600647,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 1.2608,
      "step": 135
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.4324166774749756,
      "learning_rate": 2.8e-05,
      "loss": 1.2431,
      "step": 136
    },
    {
      "epoch": 2.74,
      "grad_norm": 1.6774024963378906,
      "learning_rate": 2.6e-05,
      "loss": 1.2754,
      "step": 137
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.7049216032028198,
      "learning_rate": 2.3999999999999997e-05,
      "loss": 1.3013,
      "step": 138
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 2.0377047061920166,
      "learning_rate": 2.2e-05,
      "loss": 1.2152,
      "step": 139
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.8464957475662231,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 0.8798,
      "step": 140
    },
    {
      "epoch": 2.82,
      "grad_norm": 1.7904529571533203,
      "learning_rate": 1.7999999999999997e-05,
      "loss": 1.1148,
      "step": 141
    },
    {
      "epoch": 2.84,
      "grad_norm": 2.209272861480713,
      "learning_rate": 1.6e-05,
      "loss": 1.304,
      "step": 142
    },
    {
      "epoch": 2.86,
      "grad_norm": 1.9372332096099854,
      "learning_rate": 1.4e-05,
      "loss": 1.1654,
      "step": 143
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.8295667171478271,
      "learning_rate": 1.1999999999999999e-05,
      "loss": 1.2593,
      "step": 144
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.751671552658081,
      "learning_rate": 9.999999999999999e-06,
      "loss": 1.163,
      "step": 145
    },
    {
      "epoch": 2.92,
      "grad_norm": 1.6093651056289673,
      "learning_rate": 8e-06,
      "loss": 1.1191,
      "step": 146
    },
    {
      "epoch": 2.94,
      "grad_norm": 1.9825036525726318,
      "learning_rate": 5.999999999999999e-06,
      "loss": 1.1784,
      "step": 147
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.848048210144043,
      "learning_rate": 4e-06,
      "loss": 1.3408,
      "step": 148
    },
    {
      "epoch": 2.98,
      "grad_norm": 1.5438435077667236,
      "learning_rate": 2e-06,
      "loss": 1.0618,
      "step": 149
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.716981053352356,
      "learning_rate": 0.0,
      "loss": 1.087,
      "step": 150
    }
  ],
  "logging_steps": 1,
  "max_steps": 150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1456393995616256e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}

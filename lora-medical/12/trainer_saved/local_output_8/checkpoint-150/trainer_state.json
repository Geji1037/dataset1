{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 0.5949058532714844,
      "learning_rate": 0.000298,
      "loss": 1.8207,
      "step": 1
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.6341513395309448,
      "learning_rate": 0.000296,
      "loss": 1.8172,
      "step": 2
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.6875624656677246,
      "learning_rate": 0.000294,
      "loss": 1.7984,
      "step": 3
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.7827069759368896,
      "learning_rate": 0.000292,
      "loss": 1.4233,
      "step": 4
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.83174729347229,
      "learning_rate": 0.00029,
      "loss": 1.4901,
      "step": 5
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.7860815525054932,
      "learning_rate": 0.00028799999999999995,
      "loss": 1.5907,
      "step": 6
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.9660940766334534,
      "learning_rate": 0.00028599999999999996,
      "loss": 1.4966,
      "step": 7
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.5719563961029053,
      "learning_rate": 0.00028399999999999996,
      "loss": 1.635,
      "step": 8
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.3621091842651367,
      "learning_rate": 0.00028199999999999997,
      "loss": 1.5046,
      "step": 9
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.1784908771514893,
      "learning_rate": 0.00028,
      "loss": 1.3682,
      "step": 10
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.379843831062317,
      "learning_rate": 0.000278,
      "loss": 1.7126,
      "step": 11
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.2583962678909302,
      "learning_rate": 0.000276,
      "loss": 1.6059,
      "step": 12
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.0262528657913208,
      "learning_rate": 0.000274,
      "loss": 1.4561,
      "step": 13
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.8030344247817993,
      "learning_rate": 0.00027199999999999994,
      "loss": 1.3898,
      "step": 14
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.6307653188705444,
      "learning_rate": 0.00027,
      "loss": 1.522,
      "step": 15
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.3926417827606201,
      "learning_rate": 0.00026799999999999995,
      "loss": 1.3851,
      "step": 16
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.1321053504943848,
      "learning_rate": 0.000266,
      "loss": 1.4312,
      "step": 17
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.2205336093902588,
      "learning_rate": 0.00026399999999999997,
      "loss": 1.6152,
      "step": 18
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.3734551668167114,
      "learning_rate": 0.00026199999999999997,
      "loss": 1.5735,
      "step": 19
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.3685979843139648,
      "learning_rate": 0.00026,
      "loss": 1.5928,
      "step": 20
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.183089017868042,
      "learning_rate": 0.000258,
      "loss": 1.6273,
      "step": 21
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.1504250764846802,
      "learning_rate": 0.000256,
      "loss": 1.4374,
      "step": 22
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.1457116603851318,
      "learning_rate": 0.000254,
      "loss": 1.7752,
      "step": 23
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.963019609451294,
      "learning_rate": 0.00025199999999999995,
      "loss": 1.3965,
      "step": 24
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.1312434673309326,
      "learning_rate": 0.00025,
      "loss": 1.4754,
      "step": 25
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.3363444805145264,
      "learning_rate": 0.00024799999999999996,
      "loss": 1.3204,
      "step": 26
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.1077351570129395,
      "learning_rate": 0.00024599999999999996,
      "loss": 1.5391,
      "step": 27
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.9284582734107971,
      "learning_rate": 0.000244,
      "loss": 1.4586,
      "step": 28
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.2862675189971924,
      "learning_rate": 0.00024199999999999997,
      "loss": 1.627,
      "step": 29
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.121654748916626,
      "learning_rate": 0.00023999999999999998,
      "loss": 1.0792,
      "step": 30
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.9487617611885071,
      "learning_rate": 0.00023799999999999998,
      "loss": 1.1941,
      "step": 31
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.2131707668304443,
      "learning_rate": 0.00023599999999999996,
      "loss": 1.1246,
      "step": 32
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.096082329750061,
      "learning_rate": 0.000234,
      "loss": 1.6355,
      "step": 33
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.0378533601760864,
      "learning_rate": 0.00023199999999999997,
      "loss": 1.3988,
      "step": 34
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.1325000524520874,
      "learning_rate": 0.00023,
      "loss": 1.6477,
      "step": 35
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.006155252456665,
      "learning_rate": 0.00022799999999999999,
      "loss": 1.4985,
      "step": 36
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.225014090538025,
      "learning_rate": 0.00022599999999999996,
      "loss": 1.3201,
      "step": 37
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.9298072457313538,
      "learning_rate": 0.000224,
      "loss": 1.4798,
      "step": 38
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.091097354888916,
      "learning_rate": 0.00022199999999999998,
      "loss": 1.4527,
      "step": 39
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.1002013683319092,
      "learning_rate": 0.00021999999999999995,
      "loss": 1.3993,
      "step": 40
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.2177090644836426,
      "learning_rate": 0.00021799999999999999,
      "loss": 1.4881,
      "step": 41
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.2485849857330322,
      "learning_rate": 0.00021599999999999996,
      "loss": 1.3162,
      "step": 42
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.2103517055511475,
      "learning_rate": 0.000214,
      "loss": 1.5774,
      "step": 43
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.0488944053649902,
      "learning_rate": 0.00021199999999999998,
      "loss": 1.4459,
      "step": 44
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.0534005165100098,
      "learning_rate": 0.00020999999999999998,
      "loss": 1.3082,
      "step": 45
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.0275142192840576,
      "learning_rate": 0.000208,
      "loss": 1.422,
      "step": 46
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.109215259552002,
      "learning_rate": 0.00020599999999999997,
      "loss": 1.4806,
      "step": 47
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.9844921231269836,
      "learning_rate": 0.000204,
      "loss": 1.2654,
      "step": 48
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.3707547187805176,
      "learning_rate": 0.00020199999999999998,
      "loss": 1.2611,
      "step": 49
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.1409412622451782,
      "learning_rate": 0.00019999999999999998,
      "loss": 1.5492,
      "step": 50
    },
    {
      "epoch": 1.02,
      "grad_norm": 1.2890446186065674,
      "learning_rate": 0.000198,
      "loss": 1.427,
      "step": 51
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.9452160596847534,
      "learning_rate": 0.00019599999999999997,
      "loss": 1.0795,
      "step": 52
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.9820953011512756,
      "learning_rate": 0.00019399999999999997,
      "loss": 1.3147,
      "step": 53
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.1103515625,
      "learning_rate": 0.00019199999999999998,
      "loss": 1.4619,
      "step": 54
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.2475711107254028,
      "learning_rate": 0.00018999999999999998,
      "loss": 1.2218,
      "step": 55
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.0536270141601562,
      "learning_rate": 0.000188,
      "loss": 1.3811,
      "step": 56
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.1121299266815186,
      "learning_rate": 0.000186,
      "loss": 1.2401,
      "step": 57
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.139672040939331,
      "learning_rate": 0.00018399999999999997,
      "loss": 1.4073,
      "step": 58
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.250697374343872,
      "learning_rate": 0.00018199999999999998,
      "loss": 1.2487,
      "step": 59
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.179863691329956,
      "learning_rate": 0.00017999999999999998,
      "loss": 1.2819,
      "step": 60
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.9003586173057556,
      "learning_rate": 0.000178,
      "loss": 1.246,
      "step": 61
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.577494740486145,
      "learning_rate": 0.000176,
      "loss": 1.0468,
      "step": 62
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.3305847644805908,
      "learning_rate": 0.00017399999999999997,
      "loss": 1.3164,
      "step": 63
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.520218849182129,
      "learning_rate": 0.000172,
      "loss": 1.3216,
      "step": 64
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.4300506114959717,
      "learning_rate": 0.00016999999999999999,
      "loss": 1.424,
      "step": 65
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.5600659847259521,
      "learning_rate": 0.000168,
      "loss": 1.1543,
      "step": 66
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.168944239616394,
      "learning_rate": 0.000166,
      "loss": 1.1731,
      "step": 67
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.5976063013076782,
      "learning_rate": 0.00016399999999999997,
      "loss": 1.3162,
      "step": 68
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.713267207145691,
      "learning_rate": 0.000162,
      "loss": 1.2292,
      "step": 69
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.2652463912963867,
      "learning_rate": 0.00015999999999999999,
      "loss": 1.2838,
      "step": 70
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.35618257522583,
      "learning_rate": 0.00015799999999999996,
      "loss": 1.3918,
      "step": 71
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.2078051567077637,
      "learning_rate": 0.000156,
      "loss": 1.3037,
      "step": 72
    },
    {
      "epoch": 1.46,
      "grad_norm": 1.4026027917861938,
      "learning_rate": 0.00015399999999999998,
      "loss": 1.4351,
      "step": 73
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.4022773504257202,
      "learning_rate": 0.000152,
      "loss": 1.1923,
      "step": 74
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.3103022575378418,
      "learning_rate": 0.00015,
      "loss": 1.341,
      "step": 75
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.773866057395935,
      "learning_rate": 0.000148,
      "loss": 1.0459,
      "step": 76
    },
    {
      "epoch": 1.54,
      "grad_norm": 1.5913288593292236,
      "learning_rate": 0.000146,
      "loss": 1.234,
      "step": 77
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.3931912183761597,
      "learning_rate": 0.00014399999999999998,
      "loss": 1.452,
      "step": 78
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.5570176839828491,
      "learning_rate": 0.00014199999999999998,
      "loss": 1.3821,
      "step": 79
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.6571269035339355,
      "learning_rate": 0.00014,
      "loss": 1.2943,
      "step": 80
    },
    {
      "epoch": 1.62,
      "grad_norm": 1.6070365905761719,
      "learning_rate": 0.000138,
      "loss": 1.2718,
      "step": 81
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.6821781396865845,
      "learning_rate": 0.00013599999999999997,
      "loss": 1.5092,
      "step": 82
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.6632390022277832,
      "learning_rate": 0.00013399999999999998,
      "loss": 1.3277,
      "step": 83
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.1674209833145142,
      "learning_rate": 0.00013199999999999998,
      "loss": 1.063,
      "step": 84
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.2763113975524902,
      "learning_rate": 0.00013,
      "loss": 1.1127,
      "step": 85
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.2702457904815674,
      "learning_rate": 0.000128,
      "loss": 1.1702,
      "step": 86
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.531016230583191,
      "learning_rate": 0.00012599999999999997,
      "loss": 1.1558,
      "step": 87
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.9563729763031006,
      "learning_rate": 0.00012399999999999998,
      "loss": 1.2006,
      "step": 88
    },
    {
      "epoch": 1.78,
      "grad_norm": 1.3139668703079224,
      "learning_rate": 0.000122,
      "loss": 1.1491,
      "step": 89
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.469925045967102,
      "learning_rate": 0.00011999999999999999,
      "loss": 1.1871,
      "step": 90
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.3362880945205688,
      "learning_rate": 0.00011799999999999998,
      "loss": 1.2518,
      "step": 91
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.3189117908477783,
      "learning_rate": 0.00011599999999999999,
      "loss": 1.2692,
      "step": 92
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1.4293686151504517,
      "learning_rate": 0.00011399999999999999,
      "loss": 1.3787,
      "step": 93
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.2995821237564087,
      "learning_rate": 0.000112,
      "loss": 1.5558,
      "step": 94
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.4048210382461548,
      "learning_rate": 0.00010999999999999998,
      "loss": 1.4085,
      "step": 95
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.8259507417678833,
      "learning_rate": 0.00010799999999999998,
      "loss": 1.1588,
      "step": 96
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.345029354095459,
      "learning_rate": 0.00010599999999999999,
      "loss": 1.1819,
      "step": 97
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.5285704135894775,
      "learning_rate": 0.000104,
      "loss": 1.249,
      "step": 98
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.5816304683685303,
      "learning_rate": 0.000102,
      "loss": 1.3066,
      "step": 99
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.6259288787841797,
      "learning_rate": 9.999999999999999e-05,
      "loss": 1.4361,
      "step": 100
    },
    {
      "epoch": 2.02,
      "grad_norm": 2.1000759601593018,
      "learning_rate": 9.799999999999998e-05,
      "loss": 0.9474,
      "step": 101
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.7260899543762207,
      "learning_rate": 9.599999999999999e-05,
      "loss": 1.4927,
      "step": 102
    },
    {
      "epoch": 2.06,
      "grad_norm": 1.388511300086975,
      "learning_rate": 9.4e-05,
      "loss": 1.1855,
      "step": 103
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.162075161933899,
      "learning_rate": 9.199999999999999e-05,
      "loss": 1.2717,
      "step": 104
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.6387455463409424,
      "learning_rate": 8.999999999999999e-05,
      "loss": 1.0035,
      "step": 105
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.231313705444336,
      "learning_rate": 8.8e-05,
      "loss": 0.787,
      "step": 106
    },
    {
      "epoch": 2.14,
      "grad_norm": 1.468624472618103,
      "learning_rate": 8.6e-05,
      "loss": 1.2059,
      "step": 107
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.341522216796875,
      "learning_rate": 8.4e-05,
      "loss": 1.1176,
      "step": 108
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.5570526123046875,
      "learning_rate": 8.199999999999999e-05,
      "loss": 1.246,
      "step": 109
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.829071283340454,
      "learning_rate": 7.999999999999999e-05,
      "loss": 1.0385,
      "step": 110
    },
    {
      "epoch": 2.22,
      "grad_norm": 1.692429780960083,
      "learning_rate": 7.8e-05,
      "loss": 1.0395,
      "step": 111
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.3696000576019287,
      "learning_rate": 7.6e-05,
      "loss": 1.2834,
      "step": 112
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.6073806285858154,
      "learning_rate": 7.4e-05,
      "loss": 1.0095,
      "step": 113
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.2243733406066895,
      "learning_rate": 7.199999999999999e-05,
      "loss": 1.0386,
      "step": 114
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.8931654691696167,
      "learning_rate": 7e-05,
      "loss": 1.0908,
      "step": 115
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.627145767211914,
      "learning_rate": 6.799999999999999e-05,
      "loss": 1.4726,
      "step": 116
    },
    {
      "epoch": 2.34,
      "grad_norm": 1.7822691202163696,
      "learning_rate": 6.599999999999999e-05,
      "loss": 0.9632,
      "step": 117
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.8089617490768433,
      "learning_rate": 6.4e-05,
      "loss": 1.0555,
      "step": 118
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.3841233253479004,
      "learning_rate": 6.199999999999999e-05,
      "loss": 1.1826,
      "step": 119
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.6374648809432983,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 1.1904,
      "step": 120
    },
    {
      "epoch": 2.42,
      "grad_norm": 1.5350046157836914,
      "learning_rate": 5.7999999999999994e-05,
      "loss": 0.9448,
      "step": 121
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.9377996921539307,
      "learning_rate": 5.6e-05,
      "loss": 1.1409,
      "step": 122
    },
    {
      "epoch": 2.46,
      "grad_norm": 1.9484580755233765,
      "learning_rate": 5.399999999999999e-05,
      "loss": 1.3071,
      "step": 123
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.3742270469665527,
      "learning_rate": 5.2e-05,
      "loss": 1.405,
      "step": 124
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.5670764446258545,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 1.293,
      "step": 125
    },
    {
      "epoch": 2.52,
      "grad_norm": 2.1382076740264893,
      "learning_rate": 4.7999999999999994e-05,
      "loss": 1.1604,
      "step": 126
    },
    {
      "epoch": 2.54,
      "grad_norm": 2.1478641033172607,
      "learning_rate": 4.599999999999999e-05,
      "loss": 1.0433,
      "step": 127
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.425682544708252,
      "learning_rate": 4.4e-05,
      "loss": 1.183,
      "step": 128
    },
    {
      "epoch": 2.58,
      "grad_norm": 1.6384485960006714,
      "learning_rate": 4.2e-05,
      "loss": 1.0718,
      "step": 129
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.7269535064697266,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 1.1605,
      "step": 130
    },
    {
      "epoch": 2.62,
      "grad_norm": 1.5451501607894897,
      "learning_rate": 3.8e-05,
      "loss": 1.1176,
      "step": 131
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.6295750141143799,
      "learning_rate": 3.5999999999999994e-05,
      "loss": 1.2912,
      "step": 132
    },
    {
      "epoch": 2.66,
      "grad_norm": 1.565629243850708,
      "learning_rate": 3.399999999999999e-05,
      "loss": 0.9544,
      "step": 133
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.512033224105835,
      "learning_rate": 3.2e-05,
      "loss": 1.0973,
      "step": 134
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.7562111616134644,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 1.208,
      "step": 135
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.8154698610305786,
      "learning_rate": 2.8e-05,
      "loss": 1.2248,
      "step": 136
    },
    {
      "epoch": 2.74,
      "grad_norm": 2.121943712234497,
      "learning_rate": 2.6e-05,
      "loss": 1.026,
      "step": 137
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.9416977167129517,
      "learning_rate": 2.3999999999999997e-05,
      "loss": 1.3287,
      "step": 138
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 1.592625379562378,
      "learning_rate": 2.2e-05,
      "loss": 1.1852,
      "step": 139
    },
    {
      "epoch": 2.8,
      "grad_norm": 2.014732599258423,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 1.1709,
      "step": 140
    },
    {
      "epoch": 2.82,
      "grad_norm": 1.5318626165390015,
      "learning_rate": 1.7999999999999997e-05,
      "loss": 1.0493,
      "step": 141
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.514109492301941,
      "learning_rate": 1.6e-05,
      "loss": 1.2276,
      "step": 142
    },
    {
      "epoch": 2.86,
      "grad_norm": 1.6125590801239014,
      "learning_rate": 1.4e-05,
      "loss": 0.9549,
      "step": 143
    },
    {
      "epoch": 2.88,
      "grad_norm": 2.1390504837036133,
      "learning_rate": 1.1999999999999999e-05,
      "loss": 1.1959,
      "step": 144
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.4539794921875,
      "learning_rate": 9.999999999999999e-06,
      "loss": 1.1986,
      "step": 145
    },
    {
      "epoch": 2.92,
      "grad_norm": 1.687706708908081,
      "learning_rate": 8e-06,
      "loss": 1.3111,
      "step": 146
    },
    {
      "epoch": 2.94,
      "grad_norm": 1.7229408025741577,
      "learning_rate": 5.999999999999999e-06,
      "loss": 1.2899,
      "step": 147
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.7864233255386353,
      "learning_rate": 4e-06,
      "loss": 1.2413,
      "step": 148
    },
    {
      "epoch": 2.98,
      "grad_norm": 1.8326594829559326,
      "learning_rate": 2e-06,
      "loss": 1.3862,
      "step": 149
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.9708178043365479,
      "learning_rate": 0.0,
      "loss": 1.1952,
      "step": 150
    }
  ],
  "logging_steps": 1,
  "max_steps": 150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0879754056237056e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}

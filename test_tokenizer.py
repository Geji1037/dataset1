#!/usr/bin/env python3
"""
Tokené•¿åº¦åˆ†æè„šæœ¬ - ç”¨äºæ¯”è¾ƒFLANæ•°æ®å’ŒåŒ»ç–—æ•°æ®çš„tokenizationæ•ˆæœ
"""
import json
import os
import sys
from pathlib import Path

def test_tokenizer():
    try:
        from transformers import AutoTokenizer
        print("âœ… transformersåº“å·²å®‰è£…")
        
        # å°è¯•åŠ è½½tokenizer
        model_path = "/root/.cache/modelscope/hub/models/LLM-Research/Meta-Llama-3-8B-Instruct"
        if os.path.exists(model_path):
            print(f"âœ… æ¨¡å‹è·¯å¾„å­˜åœ¨: {model_path}")
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            tokenizer.pad_token_id = 0
            tokenizer.padding_side = "left"
            print("âœ… TokenizeråŠ è½½æˆåŠŸ")
        else:
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            print("ä½¿ç”¨åœ¨çº¿æ¨¡å‹...")
            tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", trust_remote_code=True)
            tokenizer.pad_token_id = 0
            tokenizer.padding_side = "left"
            print("âœ… åœ¨çº¿TokenizeråŠ è½½æˆåŠŸ")
            
    except Exception as e:
        print(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
        return False
    
    # æ·»åŠ prompter
    sys.path.append('.')
    try:
        from utils.prompter import Prompter
        print("âœ… PrompteråŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ PrompteråŠ è½½å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•æ•°æ®
    print("\n" + "="*50)
    print("ğŸ“Š æ•°æ®Tokené•¿åº¦åˆ†æ")
    print("="*50)
    
    # 1. æµ‹è¯•FLANæ•°æ®
    try:
        with open('data/dataset1/8/local_training_0.json', 'r') as f:
            flan_data = json.load(f)
        
        flan_sample = flan_data[0]
        prompter_alpaca = Prompter('alpaca_short')
        
        flan_full_prompt = prompter_alpaca.generate_prompt(
            flan_sample["instruction"], 
            flan_sample.get("input", None), 
            flan_sample["output"]
        )
        flan_tokens = tokenizer(flan_full_prompt, return_tensors="pt")
        flan_token_count = len(flan_tokens["input_ids"][0])
        
        print(f"ğŸ“‹ FLANæ•°æ®æ ·æœ¬:")
        print(f"   - åŸå§‹instructioné•¿åº¦: {len(flan_sample['instruction'])} å­—ç¬¦")
        print(f"   - åŸå§‹outputé•¿åº¦: {len(flan_sample['output'])} å­—ç¬¦")
        print(f"   - å®Œæ•´prompté•¿åº¦: {len(flan_full_prompt)} å­—ç¬¦")
        print(f"   - Tokenæ•°é‡: {flan_token_count}")
        print(f"   - ç¤ºä¾‹prompt: {flan_full_prompt[:200]}...")
        
    except Exception as e:
        print(f"âŒ FLANæ•°æ®æµ‹è¯•å¤±è´¥: {e}")
    
    # 2. æµ‹è¯•åŒ»ç–—æ•°æ®
    try:
        with open('dataset1/dataset1/12/local_training_0.json', 'r') as f:
            medical_data = json.load(f)
        
        medical_sample = medical_data[0]
        prompter_medical = Prompter('medical_expert_short')
        
        # æ–¹æ¡ˆ1: åªä½¿ç”¨Response
        medical_full_prompt_1 = prompter_medical.generate_prompt(
            medical_sample["Question"], 
            None, 
            medical_sample["Response"]
        )
        medical_tokens_1 = tokenizer(medical_full_prompt_1, return_tensors="pt")
        medical_token_count_1 = len(medical_tokens_1["input_ids"][0])
        
        # æ–¹æ¡ˆ2: ä½¿ç”¨CoT + Response
        combined_output = medical_sample["Complex_CoT"] + "\n\n" + medical_sample["Response"]
        medical_full_prompt_2 = prompter_medical.generate_prompt(
            medical_sample["Question"], 
            None, 
            combined_output
        )
        medical_tokens_2 = tokenizer(medical_full_prompt_2, return_tensors="pt")
        medical_token_count_2 = len(medical_tokens_2["input_ids"][0])
        
        print(f"\nğŸ¥ åŒ»ç–—æ•°æ®æ ·æœ¬:")
        print(f"   - åŸå§‹Questioné•¿åº¦: {len(medical_sample['Question'])} å­—ç¬¦")
        print(f"   - åŸå§‹Responseé•¿åº¦: {len(medical_sample['Response'])} å­—ç¬¦")
        print(f"   - åŸå§‹CoTé•¿åº¦: {len(medical_sample['Complex_CoT'])} å­—ç¬¦")
        
        print(f"\n   æ–¹æ¡ˆ1 (åªç”¨Response):")
        print(f"   - å®Œæ•´prompté•¿åº¦: {len(medical_full_prompt_1)} å­—ç¬¦")
        print(f"   - Tokenæ•°é‡: {medical_token_count_1}")
        
        print(f"\n   æ–¹æ¡ˆ2 (CoT + Response):")
        print(f"   - å®Œæ•´prompté•¿åº¦: {len(medical_full_prompt_2)} å­—ç¬¦")
        print(f"   - Tokenæ•°é‡: {medical_token_count_2}")
        
        print(f"\n   ç¤ºä¾‹prompt: {medical_full_prompt_1[:300]}...")
        
    except Exception as e:
        print(f"âŒ åŒ»ç–—æ•°æ®æµ‹è¯•å¤±è´¥: {e}")
    
    # 3. åˆ†æå’Œå»ºè®®
    print(f"\n" + "="*50)
    print("ğŸ“ˆ åˆ†æç»“æœ")
    print("="*50)
    
    try:
        ratio = medical_token_count_1 / flan_token_count
        print(f"ğŸ” Tokené•¿åº¦æ¯”è¾ƒ:")
        print(f"   - FLANæ•°æ®: {flan_token_count} tokens")
        print(f"   - åŒ»ç–—æ•°æ®(æ–¹æ¡ˆ1): {medical_token_count_1} tokens")
        print(f"   - åŒ»ç–—æ•°æ®(æ–¹æ¡ˆ2): {medical_token_count_2} tokens")
        print(f"   - æ¯”ä¾‹: åŒ»ç–—æ•°æ®æ˜¯FLANæ•°æ®çš„ {ratio:.1f} å€")
        
        cutoff_512 = 512
        cutoff_1024 = 1024
        
        print(f"\nâš ï¸  Cutoffé•¿åº¦åˆ†æ:")
        print(f"   - cutoff_len=512: FLANæ•°æ®{'âœ…é€‚åˆ' if flan_token_count <= cutoff_512 else 'âŒè¶…å‡º'}")
        print(f"   - cutoff_len=512: åŒ»ç–—æ•°æ®æ–¹æ¡ˆ1{'âœ…é€‚åˆ' if medical_token_count_1 <= cutoff_512 else 'âŒè¶…å‡º'}")
        print(f"   - cutoff_len=512: åŒ»ç–—æ•°æ®æ–¹æ¡ˆ2{'âœ…é€‚åˆ' if medical_token_count_2 <= cutoff_512 else 'âŒè¶…å‡º'}")
        
        print(f"\n   - cutoff_len=1024: åŒ»ç–—æ•°æ®æ–¹æ¡ˆ1{'âœ…é€‚åˆ' if medical_token_count_1 <= cutoff_1024 else 'âŒè¶…å‡º'}")
        print(f"   - cutoff_len=1024: åŒ»ç–—æ•°æ®æ–¹æ¡ˆ2{'âœ…é€‚åˆ' if medical_token_count_2 <= cutoff_1024 else 'âŒè¶…å‡º'}")
        
        print(f"\nğŸ’¡ å»ºè®®:")
        if medical_token_count_1 > cutoff_512:
            print(f"   - å»ºè®®å°†cutoff_lenä»512å¢åŠ åˆ°è‡³å°‘{medical_token_count_1 + 50}")
        if medical_token_count_1 > cutoff_1024:
            print(f"   - åŒ»ç–—æ•°æ®è¿‡é•¿ï¼Œå»ºè®®æˆªæ–­æˆ–åˆ†æ®µå¤„ç†")
        
        estimated_memory_ratio = (medical_token_count_1 / flan_token_count) ** 2
        print(f"   - é¢„ä¼°æ˜¾å­˜ä½¿ç”¨å¢åŠ : {estimated_memory_ratio:.1f}å€")
        print(f"   - å»ºè®®batch_sizeå‡å°‘åˆ°åŸæ¥çš„1/{int(estimated_memory_ratio)}")
        
    except:
        print("âŒ åˆ†æè®¡ç®—å¤±è´¥")
    
    return True

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹Tokené•¿åº¦åˆ†æ...")
    success = test_tokenizer()
    if success:
        print("\nâœ… åˆ†æå®Œæˆ!")
    else:
        print("\nâŒ åˆ†æå¤±è´¥!")